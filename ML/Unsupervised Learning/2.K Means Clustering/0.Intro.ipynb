{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bd1c2",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸŸ¢ **K-Means Clustering Explained**\n",
    "\n",
    "### ğŸ”¹ What is it?\n",
    "\n",
    "* **K-Means** is an **unsupervised machine learning algorithm**.\n",
    "* It groups data points into **k clusters** (where *k* is a number you choose).\n",
    "* Each cluster has a **centroid** (the \"center\" of that cluster).\n",
    "* The goal is to assign points to clusters such that **within-cluster similarity is high** and **between-cluster similarity is low**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ The Algorithm (Step-by-Step)\n",
    "\n",
    "1. **Choose the number of clusters (k).**\n",
    "\n",
    "   * Example: If you want to group customers into 3 categories, set `k=3`.\n",
    "\n",
    "2. **Initialize centroids randomly.**\n",
    "\n",
    "   * Pick *k* random points from the dataset as starting centroids.\n",
    "\n",
    "3. **Assign each data point to the nearest centroid.**\n",
    "\n",
    "   * Use a distance metric (commonly **Euclidean distance**) to find which centroid is closest.\n",
    "\n",
    "4. **Update centroids.**\n",
    "\n",
    "   * For each cluster, compute the **mean** of all points in that cluster.\n",
    "   * Move the centroid to this mean position.\n",
    "\n",
    "5. **Repeat steps 3â€“4 until convergence.**\n",
    "\n",
    "   * Clusters stop changing (or changes are very small).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Example Intuition\n",
    "\n",
    "Imagine you have 2D points that form two groups.\n",
    "\n",
    "* First, you randomly drop two centroids.\n",
    "* Each point joins whichever centroid is closer.\n",
    "* Then you shift the centroids to the average of their points.\n",
    "* Repeat â†’ until centroids settle in the middle of each group.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Choosing **k** (Number of Clusters)\n",
    "\n",
    "This is tricky! Some methods:\n",
    "\n",
    "* **Elbow Method:** Plot error (SSE) vs. k, look for the \"elbow.\"\n",
    "* **Silhouette Score:** Measures how well points fit in their cluster vs. others.\n",
    "* **Domain Knowledge:** Use real-world understanding.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Advantages\n",
    "\n",
    "âœ… Simple and fast\n",
    "âœ… Works well with large datasets\n",
    "âœ… Easy to implement\n",
    "\n",
    "### ğŸ”¹ Limitations\n",
    "\n",
    "âŒ Must predefine k\n",
    "âŒ Sensitive to initialization (different runs â†’ different results)\n",
    "âŒ Struggles with non-spherical clusters or different densities\n",
    "âŒ Affected by outliers\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Mathematical Formulation\n",
    "\n",
    "* Objective: Minimize the **within-cluster sum of squared distances**:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^k \\sum_{x \\in C_i} ||x - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $C_i$ = cluster i\n",
    "* $\\mu_i$ = centroid of cluster i\n",
    "* $x$ = data points\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "K-Means = *Initialize centroids â†’ Assign points â†’ Update centroids â†’ Repeat â†’ Stable clusters*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a522a",
   "metadata": {},
   "source": [
    "### **Dimensionality Reduction**\n",
    "\n",
    "* **Why do it?**\n",
    "\n",
    "  1. Prevent the *curse of dimensionality* (too many features hurt model performance).\n",
    "  2. Improve model training efficiency and accuracy.\n",
    "  3. Enable visualization (humans can only see up to 3D).\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Selection**\n",
    "\n",
    "* Goal: Select the most important features that strongly impact the target.\n",
    "* Methods:\n",
    "\n",
    "  * Use **covariance** and **correlation** (e.g., Pearson correlation) to measure relationships between features and target.\n",
    "  * Strong positive/negative correlation â†’ feature is important.\n",
    "  * Near-zero correlation â†’ feature is unimportant and can be dropped.\n",
    "* Example:\n",
    "\n",
    "  * **House size** vs. **price** â†’ strong correlation â†’ keep.\n",
    "  * **Fountain size** vs. **price** â†’ weak correlation â†’ drop.\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Extraction**\n",
    "\n",
    "* Goal: Create new, informative features from existing ones (instead of dropping).\n",
    "* Process: Apply transformations to combine or derive features.\n",
    "* Example:\n",
    "\n",
    "  * From **room size** + **number of rooms**, derive a new feature: **house size**, which can still predict house price effectively.\n",
    "* Key point: Some information is lost, but the new feature captures the essence of the originals while reducing dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Distinction**\n",
    "\n",
    "* **Feature Selection** â†’ Choose from existing features (drop irrelevant ones).\n",
    "* **Feature Extraction** â†’ Transform existing features to create new ones.\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ In practice: both are used in dimensionality reduction before applying models or visualization (e.g., PCA for feature extraction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d195e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
