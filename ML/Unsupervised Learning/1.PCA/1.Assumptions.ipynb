{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564b01b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ”¹ 1. **Linearity**\n",
    "\n",
    "* PCA assumes that the relationships between features are **linear**.\n",
    "* It tries to find directions (principal components) that are **linear combinations** of the original features.\n",
    "* If data has **non-linear structures**, PCA might not capture them well.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. **Large Variance = Important Information**\n",
    "\n",
    "* PCA assumes that features with **higher variance** carry more useful information.\n",
    "* Dimensions with **low variance** are considered less important and often discarded.\n",
    "* This may fail if important structure is hidden in low-variance components (e.g., anomalies).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. **Mean and Covariance are Enough**\n",
    "\n",
    "* PCA uses the **covariance matrix** (or correlation matrix) to understand relationships between features.\n",
    "* It assumes that the data is well represented by just the **mean and covariance**.\n",
    "* Works best when data is approximately **Gaussian (normal distribution)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. **Features are Continuous and Correlated**\n",
    "\n",
    "* PCA works best with **continuous, numeric variables**.\n",
    "* If features are completely uncorrelated, PCA wonâ€™t find meaningful components.\n",
    "* For categorical variables, PCA isnâ€™t directly suitable (we use MCA instead).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. **Principal Components are Orthogonal**\n",
    "\n",
    "* PCA assumes that the new dimensions (principal components) are **orthogonal (uncorrelated)** to each other.\n",
    "* This may not always reflect the true structure of real-world data.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 6. **Scaling Matters**\n",
    "\n",
    "* PCA is sensitive to the **scale of features**.\n",
    "* Features with larger magnitudes dominate the variance.\n",
    "* Thatâ€™s why we usually **standardize data** (zero mean, unit variance) before applying PCA.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "PCA assumes **linearity**, **large variance = more info**, **Gaussian-like data**, **continuous correlated features**, and that **orthogonal PCs** capture the structure. Also, **feature scaling** is crucial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d41fc9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
