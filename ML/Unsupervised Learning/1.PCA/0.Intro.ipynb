{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bd1c2",
   "metadata": {},
   "source": [
    "### Summary: Curse of Dimensionality & PCA\n",
    "\n",
    "* **Topic Introduction**\n",
    "\n",
    "  * Starting with **Principal Component Analysis (PCA)**, also known as **dimensionality reduction**.\n",
    "  * Before PCA, itâ€™s important to understand the **curse of dimensionality**.\n",
    "\n",
    "* **Curse of Dimensionality**\n",
    "\n",
    "  * **Dimensionality = number of features** in a dataset.\n",
    "  * Example: Predicting **house prices** with features like size, bedrooms, bathrooms, location, etc.\n",
    "  * Adding more **important features** (e.g., from 3 â†’ 6 â†’ 15) can initially improve model accuracy.\n",
    "    As the number of dimensions (features) increases:\n",
    "    * The data becomes **sparse** (spread out).\n",
    "    * Distance and similarity measures (like Euclidean distance) become less meaningful.\n",
    "    * Models require **exponentially more data** to generalize well.\n",
    "    * Computation becomes **slower and more complex**.\n",
    "  * But beyond a point (e.g., 50, 100, 500 features), accuracy **decreases** because:\n",
    "\n",
    "    * Many features are irrelevant or redundant.\n",
    "    * The model **overfits**, gets confused, and performance degrades.\n",
    "    * Computation becomes slower and more complex.\n",
    "  * Human analogy: if you keep adding too many conditions when estimating a house price, even an expert will get confused.\n",
    "\n",
    "* **How to Overcome Curse of Dimensionality**\n",
    "\n",
    "  1. **Feature Selection** â€“ keep only the most important features.\n",
    "  2. **Feature Extraction (Dimensionality Reduction)** â€“ create new features that summarize the essence of existing ones.\n",
    "\n",
    "     * PCA is one such method.\n",
    "\n",
    "* **PCA (Principal Component Analysis)**\n",
    "\n",
    "  * Transforms the original high-dimensional features into a smaller set of new features (principal components).\n",
    "  * These new features capture most of the **variance (information)** from the original data.\n",
    "  * Helps models train faster, avoid overfitting, and perform better.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In short:\n",
    "Too many features cause the **curse of dimensionality** â†’ models get confused and accuracy drops.\n",
    "We fix this with **feature selection** or **dimensionality reduction**.\n",
    "PCA is a key dimensionality reduction technique that weâ€™ll explore in detail next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a522a",
   "metadata": {},
   "source": [
    "### **Dimensionality Reduction**\n",
    "\n",
    "* **Why do it?**\n",
    "\n",
    "  1. Prevent the *curse of dimensionality* (too many features hurt model performance).\n",
    "  2. Improve model training efficiency and accuracy.\n",
    "  3. Enable visualization (humans can only see up to 3D).\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Selection**\n",
    "\n",
    "* Goal: Select the most important features that strongly impact the target.\n",
    "* Methods:\n",
    "\n",
    "  * Use **covariance** and **correlation** (e.g., Pearson correlation) to measure relationships between features and target.\n",
    "  * Strong positive/negative correlation â†’ feature is important.\n",
    "  * Near-zero correlation â†’ feature is unimportant and can be dropped.\n",
    "* Example:\n",
    "\n",
    "  * **House size** vs. **price** â†’ strong correlation â†’ keep.\n",
    "  * **Fountain size** vs. **price** â†’ weak correlation â†’ drop.\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Extraction**\n",
    "\n",
    "* Goal: Create new, informative features from existing ones (instead of dropping).\n",
    "* Process: Apply transformations to combine or derive features.\n",
    "* Example:\n",
    "\n",
    "  * From **room size** + **number of rooms**, derive a new feature: **house size**, which can still predict house price effectively.\n",
    "* Key point: Some information is lost, but the new feature captures the essence of the originals while reducing dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Distinction**\n",
    "\n",
    "* **Feature Selection** â†’ Choose from existing features (drop irrelevant ones).\n",
    "* **Feature Extraction** â†’ Transform existing features to create new ones.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In practice: both are used in dimensionality reduction before applying models or visualization (e.g., PCA for feature extraction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d195e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
