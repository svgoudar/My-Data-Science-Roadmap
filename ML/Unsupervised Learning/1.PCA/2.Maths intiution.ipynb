{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e942ecd",
   "metadata": {},
   "source": [
    "\n",
    "## üß† PCA Math Intuition (Step by Step)\n",
    "\n",
    "### 1. Problem Setup\n",
    "\n",
    "* We have **data in high dimensions** (say 2D: x and y).\n",
    "* Goal: reduce dimensions while keeping as much **variance (information)** as possible.\n",
    "* Why variance? Because high variance means the data is \"spread out\" along that axis ‚Üí more information is preserved.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Projection\n",
    "\n",
    "* Suppose we want to reduce 2D ‚Üí 1D.\n",
    "* Pick a direction (a **unit vector** `u`).\n",
    "* Project each data point (vector `p‚ÇÅ`, `p‚ÇÇ`, ‚Ä¶) onto `u`.\n",
    "* Projection formula:\n",
    "\n",
    "  $$\n",
    "  p_1' = (p_1 \\cdot u)\n",
    "  $$\n",
    "\n",
    "  where `¬∑` is the dot product.\n",
    "* This gives us scalar values (distances along `u`).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Variance as the Objective\n",
    "\n",
    "* After projection, we compute the **variance of these scalar projections**:\n",
    "\n",
    "  $$\n",
    "  \\text{Var}(u) = \\frac{1}{n}\\sum_{i=1}^n \\big((p_i \\cdot u) - \\overline{p \\cdot u}\\big)^2\n",
    "  $$\n",
    "* The **best direction `u`** is the one that **maximizes this variance**.\n",
    "* So PCA‚Äôs optimization problem is:\n",
    "\n",
    "  $$\n",
    "  \\max_{u}\\ \\text{Var}(u) \\quad \\text{subject to } \\|u\\|=1\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why Not Guess Directions? ‚Üí Eigenvectors\n",
    "\n",
    "* We can‚Äôt just test infinite possible `u`‚Äôs.\n",
    "* Linear algebra gives us the answer via **covariance matrix** and **eigen decomposition**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Covariance Matrix\n",
    "\n",
    "* Build covariance matrix `Œ£` from features:\n",
    "\n",
    "  $$\n",
    "  Œ£ = \\frac{1}{n} X^T X\n",
    "  $$\n",
    "\n",
    "  (after mean-centering X).\n",
    "* `Œ£[i,j]` tells how features `i` and `j` vary together.\n",
    "* Captures structure of data spread.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Eigen Decomposition\n",
    "\n",
    "* Solve:\n",
    "\n",
    "  $$\n",
    "  Œ£v = \\lambda v\n",
    "  $$\n",
    "* Where:\n",
    "\n",
    "  * `v` = **eigenvector** (direction in feature space).\n",
    "  * `Œª` = **eigenvalue** (amount of variance captured along `v`).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Selecting Principal Components\n",
    "\n",
    "* Sort eigenvalues:\n",
    "\n",
    "  $$\n",
    "  \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_d\n",
    "  $$\n",
    "* Corresponding eigenvectors = **Principal Components**.\n",
    "* PC1 = direction of max variance (`largest Œª`).\n",
    "* PC2 = next orthogonal direction of variance (`second Œª`), and so on.\n",
    "* Orthogonality ensures PCs are **uncorrelated**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Dimensionality Reduction\n",
    "\n",
    "* To reduce to `k` dimensions:\n",
    "\n",
    "  * Take top `k` eigenvectors.\n",
    "  * Form projection matrix `W = [v_1, v_2, ‚Ä¶, v_k]`.\n",
    "  * Transform data:\n",
    "\n",
    "    $$\n",
    "    Z = XW\n",
    "    $$\n",
    "* `Z` is new data in lower dimension (max variance preserved).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary in words**:\n",
    "PCA finds the **directions (principal components)** where data varies the most, using eigen decomposition of the covariance matrix. The top eigenvectors form the new axes, and projecting onto them gives compressed data with minimal information loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98265e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
