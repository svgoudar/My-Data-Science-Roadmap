{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee99926",
   "metadata": {},
   "source": [
    "Great question ðŸ‘Œ â€” letâ€™s talk about the **cost function in PCA**.\n",
    "\n",
    "PCA can be thought of in **two equivalent ways**, and each has its own **cost function**:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Maximizing Variance View (common explanation)\n",
    "\n",
    "PCA tries to find directions (**principal components**) such that when data is projected onto them, the **variance of projected data is maximized**.\n",
    "\n",
    "So the cost function for the **first principal component** is:\n",
    "\n",
    "$$\n",
    "\\max_{w} \\ \\ \\text{Var}(Xw) = \\max_{w} \\ \\ w^T S w\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "||w||^2 = 1\n",
    "$$\n",
    "\n",
    "* $X$ = data matrix (mean-centered)\n",
    "* $S = \\frac{1}{n} X^T X$ = covariance matrix\n",
    "* $w$ = unit vector (principal component direction)\n",
    "\n",
    "ðŸ‘‰ This is solved via **eigen decomposition** â€” the eigenvector with the largest eigenvalue is PC1.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Minimizing Reconstruction Error View (alternative view)\n",
    "\n",
    "PCA can also be seen as **finding a low-dimensional subspace that minimizes reconstruction error** when projecting data down and back up.\n",
    "\n",
    "If $X \\in \\mathbb{R}^{n \\times d}$ is the data and we reduce to $k$ dimensions:\n",
    "\n",
    "$$\n",
    "\\min_{W_k} \\ \\ ||X - X W_k W_k^T||_F^2\n",
    "$$\n",
    "\n",
    "* $W_k \\in \\mathbb{R}^{d \\times k}$ = matrix of top $k$ principal components\n",
    "* $|| \\cdot ||_F$ = Frobenius norm (sum of squared errors over all entries)\n",
    "\n",
    "This means PCA finds a subspace that **minimizes squared distance between the original data and its projection**.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Key connection:**\n",
    "\n",
    "* Maximizing variance â†” Minimizing reconstruction error (they are mathematically equivalent).\n",
    "* Both lead to choosing eigenvectors of covariance matrix with largest eigenvalues.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
