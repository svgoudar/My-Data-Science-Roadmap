{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bd1c2",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ”¹ Introduction\n",
    "\n",
    "* Goal: Group data points into clusters, similar to K-means, but **without centroids**.\n",
    "* Two main types:\n",
    "\n",
    "  * **Agglomerative** (bottom-up: combine small clusters into bigger ones).\n",
    "  * **Divisive** (top-down: split large cluster into smaller ones).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Steps in **Agglomerative Hierarchical Clustering**\n",
    "\n",
    "1. **Start**: Each data point is its own cluster.\n",
    "2. **Find nearest points/clusters** (using a distance metric like **Euclidean/Manhattan**).\n",
    "3. **Merge nearest clusters** into a new cluster.\n",
    "4. **Repeat** steps until all points are merged into a single cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Dendrogram\n",
    "\n",
    "* A tree-like diagram that shows how clusters are formed.\n",
    "* **X-axis** â†’ data points.\n",
    "* **Y-axis** â†’ distance (e.g., Euclidean).\n",
    "* Steps:\n",
    "\n",
    "  * Merge closest points first, then merge clusters progressively.\n",
    "  * Heights in dendrogram correspond to distances between clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Choosing Number of Clusters (k)\n",
    "\n",
    "* Decide using **distance threshold**:\n",
    "\n",
    "  * Draw a horizontal line across dendrogram at a chosen distance.\n",
    "  * Number of clusters = number of vertical lines the horizontal line cuts.\n",
    "* **Rule of thumb (hack)**:\n",
    "\n",
    "  * Select the **longest vertical line** in dendrogram that no horizontal line crosses.\n",
    "  * Draw a horizontal cut through it â†’ gives optimal number of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Key Points\n",
    "\n",
    "* **Agglomerative** = bottom-up merging.\n",
    "* **Divisive** = top-down splitting.\n",
    "* Dendrogram helps in deciding **k**.\n",
    "* Threshold determines granularity of clusters:\n",
    "\n",
    "  * Lower threshold â†’ more clusters.\n",
    "  * Higher threshold â†’ fewer clusters.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… So hierarchical clustering = **iterative merging/splitting + dendrogram for visualization + threshold to pick k.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a522a",
   "metadata": {},
   "source": [
    "### **Dimensionality Reduction**\n",
    "\n",
    "* **Why do it?**\n",
    "\n",
    "  1. Prevent the *curse of dimensionality* (too many features hurt model performance).\n",
    "  2. Improve model training efficiency and accuracy.\n",
    "  3. Enable visualization (humans can only see up to 3D).\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Selection**\n",
    "\n",
    "* Goal: Select the most important features that strongly impact the target.\n",
    "* Methods:\n",
    "\n",
    "  * Use **covariance** and **correlation** (e.g., Pearson correlation) to measure relationships between features and target.\n",
    "  * Strong positive/negative correlation â†’ feature is important.\n",
    "  * Near-zero correlation â†’ feature is unimportant and can be dropped.\n",
    "* Example:\n",
    "\n",
    "  * **House size** vs. **price** â†’ strong correlation â†’ keep.\n",
    "  * **Fountain size** vs. **price** â†’ weak correlation â†’ drop.\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Extraction**\n",
    "\n",
    "* Goal: Create new, informative features from existing ones (instead of dropping).\n",
    "* Process: Apply transformations to combine or derive features.\n",
    "* Example:\n",
    "\n",
    "  * From **room size** + **number of rooms**, derive a new feature: **house size**, which can still predict house price effectively.\n",
    "* Key point: Some information is lost, but the new feature captures the essence of the originals while reducing dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Distinction**\n",
    "\n",
    "* **Feature Selection** â†’ Choose from existing features (drop irrelevant ones).\n",
    "* **Feature Extraction** â†’ Transform existing features to create new ones.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In practice: both are used in dimensionality reduction before applying models or visualization (e.g., PCA for feature extraction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d195e",
   "metadata": {},
   "source": [
    "| Aspect           | K-Means                      | Hierarchical Clustering              |\n",
    "| ---------------- | ---------------------------- | ------------------------------------ |\n",
    "| Input needed     | k (number of clusters)       | No k needed initially                |\n",
    "| Approach         | Partition-based              | Tree (nested clusters)               |\n",
    "| Shape assumption | Spherical clusters           | Flexible shapes                      |\n",
    "| Initialization   | Random centroids (K-Means++) | Deterministic                        |\n",
    "| Complexity       | O(n Ã— k Ã— iterations)        | O(nÂ²)                                |\n",
    "| Best for         | Large datasets               | Small/medium datasets                |\n",
    "| Output           | Flat clustering              | Hierarchical clustering (dendrogram) |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
