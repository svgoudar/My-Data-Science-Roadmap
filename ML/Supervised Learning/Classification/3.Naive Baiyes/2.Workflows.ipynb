{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775e7d1f",
   "metadata": {},
   "source": [
    "### **Naive Bayes Classifier Workflow**\n",
    "\n",
    "**Purpose:** Solve **classification problems** (binary or multi-class) using **Bayes' theorem**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Training Phase (Learning from Data)**\n",
    "\n",
    "* **Calculate Prior Probabilities $P(y)$**:\n",
    "  Probability of each class occurring in the training data (e.g., `P(Yes) = 9/14`, `P(No) = 5/14`).\n",
    "* **Calculate Likelihoods $P(X_i | y)$**:\n",
    "  For each feature and class, compute conditional probabilities assuming features are independent (e.g., `P(Sunny | Yes) = 2/9`).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Prediction Phase (Classifying New Data)**\n",
    "\n",
    "* **Input:** New record with feature values (e.g., Outlook = Sunny, Temperature = Hot).\n",
    "* **Compute Posterior (numerator only):**\n",
    "  Multiply prior and likelihoods for each class:\n",
    "\n",
    "  $$\n",
    "  P(y | X_1, X_2, ...) \\propto P(y) \\cdot P(X_1 | y) \\cdot P(X_2 | y) \\cdot ...\n",
    "  $$\n",
    "* **Compare and Predict:**\n",
    "  Choose the class with the **highest probability** (Argmax).\n",
    "\n",
    "  * Example: `P(Yes) ≈ 0.031`, `P(No) ≈ 0.085` → predict **No**.\n",
    "* **Optional Normalization:**\n",
    "  Convert scores to actual probabilities summing to 1:\n",
    "\n",
    "  * `P(Yes) ≈ 0.27`, `P(No) ≈ 0.73` → confirms prediction.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key Idea:** Naive Bayes assumes **feature independence**, uses **training probabilities**, and predicts the class with the **highest posterior probability**.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
