{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c3a0fc",
   "metadata": {},
   "source": [
    "### 1. **Introduction**\n",
    "\n",
    "* Na√Øve Bayes is a machine learning algorithm for **classification** (binary and multi-class).\n",
    "* It is based on **Bayes‚Äô Theorem** from probability.\n",
    "* Understanding **probability concepts** (independent vs dependent events) is necessary before deriving Bayes‚Äô Theorem.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Probability Basics**\n",
    "\n",
    "* **Independent events**: One outcome does not affect another.\n",
    "\n",
    "  * Example: rolling a dice. Each face (1‚Äì6) has probability **1/6**.\n",
    "* **Dependent events**: One event changes the probability of another.\n",
    "\n",
    "  * Example: bag with 3 orange and 2 yellow marbles.\n",
    "\n",
    "    * Probability(orange first) = 3/5.\n",
    "    * After removing 1 orange, probability(yellow next) = 2/4 = 1/2.\n",
    "  * Joint probability = P(O and Y) = P(O) √ó P(Y|O) = 3/5 √ó 1/2 = 3/10.\n",
    "* This introduces **conditional probability**:\n",
    "\n",
    "  * P(B|A) = probability of event B given event A has occurred.\n",
    "  * General form: P(A and B) = P(A) √ó P(B|A).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Bayes‚Äô Theorem**\n",
    "\n",
    "* Derived from conditional probability symmetry:\n",
    "\n",
    "  * P(A and B) = P(A)P(B|A) = P(B)P(A|B).\n",
    "* Rearranging gives Bayes‚Äô theorem:\n",
    "\n",
    "  $$\n",
    "  P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\n",
    "  $$\n",
    "* Components:\n",
    "\n",
    "  * P(A|B): Posterior (probability of A given evidence B).\n",
    "  * P(A): Prior (probability of A).\n",
    "  * P(B|A): Likelihood.\n",
    "  * P(B): Evidence (normalization constant).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Na√Øve Bayes in Machine Learning**\n",
    "\n",
    "* Aim: Predict class $y$ given input features $X_1, X_2, ..., X_n$.\n",
    "* Formula:\n",
    "\n",
    "  $$\n",
    "  P(y | X_1, X_2, ..., X_n) = \\frac{P(y) \\cdot P(X_1, X_2, ..., X_n | y)}{P(X_1, X_2, ..., X_n)}\n",
    "  $$\n",
    "* With the **Na√Øve assumption** (features are conditionally independent given class):\n",
    "\n",
    "  $$\n",
    "  P(y | X_1, ..., X_n) \\propto P(y) \\cdot \\prod_{i=1}^n P(X_i|y)\n",
    "  $$\n",
    "* Denominator $P(X_1,...,X_n)$ is constant for all classes ‚Üí ignored in comparison.\n",
    "* Classification rule: Choose the class with **maximum posterior probability**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Worked Example (Tennis dataset)**\n",
    "\n",
    "Dataset features: Outlook, Temperature, Humidity, Wind ‚Üí Target: Play Tennis (Yes/No).\n",
    "\n",
    "**Step 1: Prior Probabilities**\n",
    "\n",
    "* Yes = 9/14.\n",
    "* No = 5/14.\n",
    "\n",
    "**Step 2: Conditional Probabilities** (example for Outlook):\n",
    "\n",
    "* P(Sunny|Yes) = 2/9, P(Overcast|Yes) = 4/9, P(Rain|Yes) = 3/9.\n",
    "* P(Sunny|No) = 3/5, P(Overcast|No) = 0, P(Rain|No) = 2/5.\n",
    "  (Similarly calculated for Temperature values: Hot, Mild, Cool.)\n",
    "\n",
    "**Step 3: Prediction for new test case**\n",
    "Test data: Outlook = Sunny, Temperature = Hot.\n",
    "\n",
    "* Compute posterior for **Yes**:\n",
    "\n",
    "  $$\n",
    "  P(Yes|Sunny,Hot) \\propto P(Yes) \\cdot P(Sunny|Yes) \\cdot P(Hot|Yes)\n",
    "  $$\n",
    "\n",
    "  \\= (9/14) √ó (2/9) √ó (2/9) ‚âà 0.031.\n",
    "\n",
    "* Compute posterior for **No**:\n",
    "\n",
    "  $$\n",
    "  P(No|Sunny,Hot) \\propto P(No) \\cdot P(Sunny|No) \\cdot P(Hot|No)\n",
    "  $$\n",
    "\n",
    "  \\= (5/14) √ó (3/5) √ó (2/5) ‚âà 0.085.\n",
    "\n",
    "* Normalize:\n",
    "\n",
    "  * Yes = 0.031 / (0.031 + 0.085) ‚âà **27%**.\n",
    "  * No = 0.085 / (0.031 + 0.085) ‚âà **73%**.\n",
    "\n",
    "**Result:** Prediction = **No (will not play tennis)** since probability is higher.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Conclusion**\n",
    "\n",
    "* Na√Øve Bayes uses **Bayes‚Äô theorem** with the assumption of **feature independence**.\n",
    "* For each class, compute **prior √ó product of likelihoods**, then compare probabilities.\n",
    "* It works for both **binary and multi-class classification**.\n",
    "* Implementation in code is simple, but understanding the derivation and probability mechanics is essential.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ee0e5",
   "metadata": {},
   "source": [
    "# üîπ 1. Hard Margin SVC\n",
    "\n",
    "* **Assumption**: The data is **perfectly linearly separable** (no overlap, no noise).\n",
    "* The goal is to find a hyperplane that **separates the two classes with no misclassification**.\n",
    "* It maximizes the margin subject to:\n",
    "\n",
    "$$\n",
    "y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "* Means: every point must be correctly classified and lie **outside the margin boundaries**.\n",
    "\n",
    "‚úÖ **Pros**:\n",
    "\n",
    "* Simple, clean, and works when data is truly separable.\n",
    "\n",
    "‚ùå **Cons**:\n",
    "\n",
    "* Very sensitive to **outliers** and **noise**.\n",
    "\n",
    "  * Even one misclassified or overlapping point can break the model.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 2. Soft Margin SVC\n",
    "\n",
    "* **Reality**: Most real-world data is **not perfectly separable** (there‚Äôs noise, overlap, outliers).\n",
    "* Soft margin allows some **violations of the margin rule** using **slack variables $\\xi_i$**.\n",
    "* Optimization problem:\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "* Here:\n",
    "\n",
    "  * $\\xi_i$ measures how much point $i$ violates the margin (or is misclassified).\n",
    "  * $C$ controls the penalty for misclassification:\n",
    "\n",
    "    * Large $C$ ‚Üí less tolerance (tries to classify every point correctly).\n",
    "    * Small $C$ ‚Üí more tolerance, allows wider margin with some misclassifications.\n",
    "\n",
    "‚úÖ **Pros**:\n",
    "\n",
    "* Works better on noisy, real-world data.\n",
    "* Balances **margin maximization** and **classification errors**.\n",
    "\n",
    "‚ùå **Cons**:\n",
    "\n",
    "* Needs tuning of **C parameter**.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 3. Quick Analogy\n",
    "\n",
    "* **Hard Margin** = \"Strict teacher\" ‚Üí *no mistakes allowed*. Even one wrong answer = fail.\n",
    "* **Soft Margin** = \"Practical teacher\" ‚Üí *a few mistakes are allowed* if the overall understanding is strong.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary**:\n",
    "\n",
    "* **Hard Margin** ‚Üí perfect separation, no misclassification, sensitive to outliers.\n",
    "* **Soft Margin** ‚Üí allows some errors (controlled by C), more robust and practical.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
