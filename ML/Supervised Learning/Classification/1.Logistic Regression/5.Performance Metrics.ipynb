{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d89746a",
   "metadata": {},
   "source": [
    "# ðŸ“Š **Performance Metrics in Logistic Regression**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Confusion Matrix**\n",
    "\n",
    "* A **summary table** of predictions vs. actual outcomes.\n",
    "\n",
    "|                     | Predicted Positive  | Predicted Negative  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "* Helps derive all other metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Accuracy**\n",
    "\n",
    "* **Overall correctness** of the model.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "* **Good when classes are balanced**, misleading if data is imbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Precision (Positive Predictive Value)**\n",
    "\n",
    "* Out of all predicted positives, how many are actually positive?\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "* Useful when **false positives are costly** (e.g., spam filters).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Recall (Sensitivity / True Positive Rate)**\n",
    "\n",
    "* Out of all actual positives, how many did we correctly predict?\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "* Useful when **false negatives are costly** (e.g., disease detection).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Specificity (True Negative Rate)**\n",
    "\n",
    "* Out of all actual negatives, how many did we correctly predict?\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "* Complements recall: **focuses on negatives**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. F1 Score**\n",
    "\n",
    "* **Harmonic mean** of precision and recall.\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "* Good when we need a **balance between precision and recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. F-beta Score**\n",
    "\n",
    "* Weighted version of F1.\n",
    "\n",
    "$$\n",
    "F_\\beta = (1+\\beta^2)\\cdot \\frac{\\text{Precision}\\cdot\\text{Recall}}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "* Î² > 1 â†’ more weight on recall.\n",
    "* Î² < 1 â†’ more weight on precision.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. ROC Curve & AUC**\n",
    "\n",
    "* **ROC Curve**: Plots **TPR (Recall)** vs. **FPR (1 - Specificity)** at different thresholds.\n",
    "* **AUC (Area Under Curve)**: Measures how well the model separates classes.\n",
    "\n",
    "  * AUC = 1 â†’ perfect model.\n",
    "  * AUC = 0.5 â†’ random guessing.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Log Loss (Cross-Entropy Loss)**\n",
    "\n",
    "* Measures how well predicted probabilities match actual labels.\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^N \\big[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\big]\n",
    "$$\n",
    "\n",
    "* Lower log loss = better model.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary:**\n",
    "\n",
    "* **Accuracy** â†’ overall correctness (works best for balanced data).\n",
    "* **Precision** â†’ good when FP is costly.\n",
    "* **Recall** â†’ good when FN is costly.\n",
    "* **F1 / F-beta** â†’ balance precision and recall.\n",
    "* **ROC-AUC** â†’ good for comparing models.\n",
    "* **Log Loss** â†’ probability-based evaluation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
