{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb009672",
   "metadata": {},
   "source": [
    "### **Why Linear Regression Fails for Classification**\n",
    "\n",
    "1. **Outliers:** The best-fit line can be heavily influenced by extreme values.\n",
    "2. **Output Range:** Linear regression can produce outputs less than 0 or greater than 1, which are invalid for probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solution: Logistic Regression**\n",
    "\n",
    "* Apply a **squashing function** to the linear output to constrain predictions between 0 and 1.\n",
    "* The **sigmoid (logistic) function** is used:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Key properties:**\n",
    "\n",
    "* Outputs always between 0 and 1.\n",
    "\n",
    "* Midpoint at 0.5 when $z = 0$.\n",
    "\n",
    "* If $z > 0$, $\\sigma(z) > 0.5$.\n",
    "\n",
    "* **Hypothesis function with sigmoid:**\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sigma(\\theta_0 + \\theta_1 x_1)\n",
    "$$\n",
    "\n",
    "* For multiple features:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sigma(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Cost Function**\n",
    "\n",
    "* Linear regression cost function leads to **non-convexity** when combined with sigmoid, causing **local minima**.\n",
    "* Logistic regression uses **log loss (cross-entropy)** for convexity:\n",
    "\n",
    "$$\n",
    "\\text{Cost}(h_\\theta(x), y) =\n",
    "\\begin{cases} \n",
    "- \\log(h_\\theta(x)) & \\text{if } y = 1 \\\\\n",
    "- \\log(1 - h_\\theta(x)) & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Combined form:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\big[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\big]\n",
    "$$\n",
    "\n",
    "* Ensures **convexity**, allowing gradient descent to reliably find the **global minimum**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient Descent**\n",
    "\n",
    "* Parameter update rule:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "* Repeat until convergence.\n",
    "* For multiple features, update each $\\theta_j$ in the same way.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "1. Fit a linear model: $\\theta_0 + \\theta_1 x_1 + \\dots$\n",
    "2. Apply **sigmoid activation** to squash outputs between 0 and 1.\n",
    "3. Use **log loss** to ensure a convex cost function.\n",
    "4. Optimize parameters using **gradient descent**.\n",
    "5. Predictions can now be interpreted as **probabilities** for classification.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
