{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4412d306",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Cost Function in SVC (Support Vector Classifier)\n",
    "\n",
    "SVC is built on the idea of finding a **decision boundary (hyperplane)** that separates classes with the **maximum margin** while allowing some misclassifications (soft margin).\n",
    "\n",
    "The **cost function** (also called the **objective function**) balances two goals:\n",
    "\n",
    "1. **Maximize the margin** â†’ keep the separating hyperplane as far as possible from the nearest points.\n",
    "2. **Minimize misclassification errors** â†’ penalize points that lie inside the margin or are misclassified.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Hard-Margin SVC (No Misclassifications)**\n",
    "\n",
    "When data is **linearly separable** (perfect split possible), the objective is:\n",
    "\n",
    "$$\n",
    "\\min_{w,b} \\ \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "* Here, $\\|w\\|$ controls the margin width (smaller $\\|w\\|$ = larger margin).\n",
    "* No slack variables â†’ strict separation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Soft-Margin SVC (Realistic Case)**\n",
    "\n",
    "When data is **not perfectly separable**, slack variables $\\xi_i \\geq 0$ are introduced to allow violations.\n",
    "\n",
    "The cost function becomes:\n",
    "\n",
    "$$\n",
    "\\min_{w,b,\\xi} \\ \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i \\quad \\forall i\n",
    "$$\n",
    "\n",
    "* First term â†’ $\\frac{1}{2} \\|w\\|^2$: keeps the margin wide (regularization).\n",
    "* Second term â†’ $C \\sum \\xi_i$: penalizes misclassifications.\n",
    "* $C$ = **regularization parameter**:\n",
    "\n",
    "  * Large $C$: prioritizes correct classification (smaller margin, risk of overfitting).\n",
    "  * Small $C$: allows more violations (larger margin, more general).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Hinge Loss (Common Interpretation)**\n",
    "\n",
    "The misclassification penalty in SVC is based on the **hinge loss**:\n",
    "\n",
    "$$\n",
    "L(y, f(x)) = \\max(0, 1 - y \\cdot f(x))\n",
    "$$\n",
    "\n",
    "where $f(x) = w \\cdot x + b$.\n",
    "\n",
    "* If the point is correctly classified and outside the margin â†’ loss = 0.\n",
    "* If itâ€™s inside the margin or misclassified â†’ loss > 0 (linear penalty).\n",
    "\n",
    "Thus, the **primal cost function** of SVC is often written as:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\max(0, 1 - y_i(w \\cdot x_i + b))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Summary**\n",
    "\n",
    "* **Hard Margin**: Minimize $\\frac{1}{2}\\|w\\|^2$ (no misclassifications).\n",
    "* **Soft Margin**: Minimize $\\frac{1}{2}\\|w\\|^2 + C \\sum \\xi_i$ (allows some errors).\n",
    "* **Hinge Loss view**: Minimize\n",
    "\n",
    "  $$\n",
    "  \\frac{1}{2}\\|w\\|^2 + C \\sum \\max(0, 1 - y_i f(x_i))\n",
    "  $$\n",
    "* $C$ controls the tradeoff between margin maximization and error tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In simple words:\n",
    "The **cost function in SVC** tries to make the margin as wide as possible while penalizing points that lie on the wrong side of the margin (using **hinge loss**).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
