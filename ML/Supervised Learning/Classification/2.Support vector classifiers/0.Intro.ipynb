{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c3a0fc",
   "metadata": {},
   "source": [
    "# üîπ Support Vector Classifier (SVC)\n",
    "\n",
    "A **Support Vector Classifier** is a supervised **classification algorithm** under the **Support Vector Machine (SVM)** family.\n",
    "It tries to find the **best decision boundary** (called a **hyperplane**) that separates data into different classes.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Core Idea**\n",
    "\n",
    "* For binary classification, data points belong to **two classes** (+1 and -1).\n",
    "* The classifier searches for a **hyperplane** that separates the two classes.\n",
    "* Among all possible hyperplanes, it chooses the one that:\n",
    "\n",
    "  * **Maximizes the margin** (the distance between the hyperplane and the nearest points of each class).\n",
    "  * Uses only the **support vectors** (the closest data points to the boundary) to define the boundary.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Linear SVC**\n",
    "\n",
    "* When classes are **linearly separable**, the classifier finds a straight line (2D) or plane (3D) or hyperplane (higher-D).\n",
    "* Example in 2D:\n",
    "\n",
    "  * Equation of hyperplane:\n",
    "\n",
    "    $$\n",
    "    w^T x + b = 0\n",
    "    $$\n",
    "  * Margin width = $\\frac{2}{||w||}$.\n",
    "  * Goal: maximize this margin.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Soft Margin SVC**\n",
    "\n",
    "* In real-world data, perfect separation isn‚Äôt always possible.\n",
    "* We allow some points to fall inside the margin or be misclassified (slack variables $\\xi_i$).\n",
    "* The **C parameter** controls this trade-off:\n",
    "\n",
    "  * High C ‚Üí strict, fewer misclassifications, smaller margin.\n",
    "  * Low C ‚Üí allows more misclassifications, larger margin.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Nonlinear SVC (Kernel Trick)**\n",
    "\n",
    "* If data is **not linearly separable**, we transform it into a higher-dimensional space using a **kernel function**.\n",
    "* Common kernels:\n",
    "\n",
    "  * Linear kernel ‚Üí simple hyperplane.\n",
    "  * Polynomial kernel ‚Üí curved boundaries.\n",
    "  * RBF (Radial Basis Function) kernel ‚Üí flexible, nonlinear boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Decision Rule**\n",
    "\n",
    "For prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(w^T x + b)\n",
    "$$\n",
    "\n",
    "For kernel SVC:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_i \\alpha_i y_i K(x_i, x) + b\n",
    "$$\n",
    "\n",
    "where only **support vectors** (points with non-zero $\\alpha_i$) contribute.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Advantages**\n",
    "\n",
    "* Effective in **high-dimensional data**.\n",
    "* Works well when classes are not linearly separable (thanks to kernels).\n",
    "* Robust against overfitting (margin maximization helps generalization).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Limitations**\n",
    "\n",
    "* Choosing the right **kernel** and tuning **C, gamma** is crucial.\n",
    "* Computationally expensive for very large datasets.\n",
    "* Does not naturally provide probability outputs (though it can be approximated).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary**:\n",
    "Support Vector Classifier finds the **optimal hyperplane** that separates classes with the **maximum margin**, relying only on **support vectors** to define the boundary. With the **kernel trick**, it can handle nonlinear decision boundaries as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ee0e5",
   "metadata": {},
   "source": [
    "# üîπ 1. Hard Margin SVC\n",
    "\n",
    "* **Assumption**: The data is **perfectly linearly separable** (no overlap, no noise).\n",
    "* The goal is to find a hyperplane that **separates the two classes with no misclassification**.\n",
    "* It maximizes the margin subject to:\n",
    "\n",
    "$$\n",
    "y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "* Means: every point must be correctly classified and lie **outside the margin boundaries**.\n",
    "\n",
    "‚úÖ **Pros**:\n",
    "\n",
    "* Simple, clean, and works when data is truly separable.\n",
    "\n",
    "‚ùå **Cons**:\n",
    "\n",
    "* Very sensitive to **outliers** and **noise**.\n",
    "\n",
    "  * Even one misclassified or overlapping point can break the model.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 2. Soft Margin SVC\n",
    "\n",
    "* **Reality**: Most real-world data is **not perfectly separable** (there‚Äôs noise, overlap, outliers).\n",
    "* Soft margin allows some **violations of the margin rule** using **slack variables $\\xi_i$**.\n",
    "* Optimization problem:\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "* Here:\n",
    "\n",
    "  * $\\xi_i$ measures how much point $i$ violates the margin (or is misclassified).\n",
    "  * $C$ controls the penalty for misclassification:\n",
    "\n",
    "    * Large $C$ ‚Üí less tolerance (tries to classify every point correctly).\n",
    "    * Small $C$ ‚Üí more tolerance, allows wider margin with some misclassifications.\n",
    "\n",
    "‚úÖ **Pros**:\n",
    "\n",
    "* Works better on noisy, real-world data.\n",
    "* Balances **margin maximization** and **classification errors**.\n",
    "\n",
    "‚ùå **Cons**:\n",
    "\n",
    "* Needs tuning of **C parameter**.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 3. Quick Analogy\n",
    "\n",
    "* **Hard Margin** = \"Strict teacher\" ‚Üí *no mistakes allowed*. Even one wrong answer = fail.\n",
    "* **Soft Margin** = \"Practical teacher\" ‚Üí *a few mistakes are allowed* if the overall understanding is strong.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary**:\n",
    "\n",
    "* **Hard Margin** ‚Üí perfect separation, no misclassification, sensitive to outliers.\n",
    "* **Soft Margin** ‚Üí allows some errors (controlled by C), more robust and practical.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
