{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e1941e",
   "metadata": {},
   "source": [
    "## ğŸ”¹ 1. Manual Search\n",
    "\n",
    "* You try hyperparameters based on **intuition/experience**.\n",
    "* Example: Set `alpha = 0.1` in Ridge Regression, check performance, then try `alpha = 1`.\n",
    "  âœ… Simple, fast.\n",
    "  âŒ Not scalable, may miss optimal values.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 2. Grid Search Cross Validation (GridSearchCV)\n",
    "\n",
    "* Exhaustively tries **all combinations** of hyperparameters.\n",
    "* Example:\n",
    "\n",
    "```python\n",
    "param_grid = {\"alpha\": [0.01, 0.1, 1, 10]}\n",
    "```\n",
    "\n",
    "It will train the model with **each value** and pick the best based on CV score.\n",
    "\n",
    "âœ… Finds best within defined grid.\n",
    "âŒ Very slow if parameter space is large.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 3. Random Search Cross Validation (RandomizedSearchCV)\n",
    "\n",
    "* Randomly selects combinations of hyperparameters from a given distribution.\n",
    "* Not every combination is tested, but sampling is efficient.\n",
    "* Example:\n",
    "\n",
    "```python\n",
    "param_dist = {\"alpha\": np.logspace(-3, 2, 100)}\n",
    "```\n",
    "\n",
    "âœ… Faster than Grid Search, good for large search spaces.\n",
    "âŒ May not find the exact global best, but usually close enough.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 4. Bayesian Optimization (e.g., Optuna, Hyperopt, skopt)\n",
    "\n",
    "* Uses **probabilistic models** (Gaussian Processes, TPE) to choose the next set of hyperparameters based on **past results**.\n",
    "* Focuses on promising regions instead of blind search.\n",
    "\n",
    "âœ… Much faster than Grid/Random Search.\n",
    "âœ… Works well in large & continuous parameter spaces.\n",
    "âŒ More complex to set up.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 5. Gradient-based Optimization (e.g., Hypergradient Descent)\n",
    "\n",
    "* Adjusts hyperparameters (like learning rate, regularization strength) using gradients of the validation error.\n",
    "* Mostly used in neural networks, not very common in basic regression.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 6. Evolutionary Algorithms / Genetic Algorithms\n",
    "\n",
    "* Mimics **natural selection**: starts with random hyperparameters, mutates, and evolves over generations.\n",
    "* Used when search space is very complex (e.g., many hyperparameters).\n",
    "\n",
    "âœ… Can handle discrete + continuous hyperparameters.\n",
    "âŒ Slower than Bayesian Optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 7. Automated Machine Learning (AutoML)\n",
    "\n",
    "* Tools like **Auto-sklearn, TPOT, H2O, Google AutoML** combine different tuning methods.\n",
    "* They search across hyperparameters and model types.\n",
    "\n",
    "âœ… Best for non-experts.\n",
    "âŒ Less control, can be resource-heavy.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Summary (Methods for Regression)\n",
    "\n",
    "| Method                | Speed â±  | Accuracy ğŸ¯        | Best For                  |\n",
    "| --------------------- | -------- | ------------------ | ------------------------- |\n",
    "| Manual Search         | Fast     | Low                | Small experiments         |\n",
    "| Grid Search CV        | Slow     | High (within grid) | Small parameter spaces    |\n",
    "| Random Search CV      | Medium   | Medium-High        | Large parameter spaces    |\n",
    "| Bayesian Optimization | Fast     | High               | Complex regression models |\n",
    "| Gradient-based        | Medium   | High (NNs)         | Deep learning regressors  |\n",
    "| Genetic Algorithms    | Slow     | Medium-High        | Very large/complex search |\n",
    "| AutoML                | Variable | High               | End-to-end automation     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788ecb8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
