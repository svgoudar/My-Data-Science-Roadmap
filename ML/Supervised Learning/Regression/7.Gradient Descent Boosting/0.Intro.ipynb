{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3fdc1d3",
   "metadata": {},
   "source": [
    "# üöÄ Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an **ensemble method** (like bagging and boosting) where models (usually **decision trees**) are trained **sequentially**. Each new model tries to correct the errors of the previous ones.\n",
    "\n",
    "But instead of just re-weighting misclassified points (like AdaBoost), Gradient Boosting uses the idea of **Gradient Descent** on a loss function.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Intuition: \"Boosting as Gradient Descent\"\n",
    "\n",
    "* Think of your **loss function** $J(y, \\hat{y})$, e.g.:\n",
    "\n",
    "  * MSE for regression:\n",
    "\n",
    "    $$\n",
    "    J = \\frac{1}{2m} \\sum (y_i - \\hat{y}_i)^2\n",
    "    $$\n",
    "  * Log Loss for classification.\n",
    "\n",
    "* Gradient Descent updates parameters by moving **opposite to the gradient** of the loss.\n",
    "\n",
    "* In Gradient Boosting, instead of directly updating parameters, we **fit a new weak learner (tree)** to predict the **negative gradient** of the loss (the \"residuals\").\n",
    "\n",
    "üëâ So Gradient Boosting = **Fitting trees to the gradient of errors** step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. The Workflow\n",
    "\n",
    "1. **Initialize**\n",
    "   Start with a simple model, e.g. predict the mean of $y$:\n",
    "\n",
    "   $$\n",
    "   F_0(x) = \\arg\\min_c \\sum L(y_i, c)\n",
    "   $$\n",
    "\n",
    "2. **Compute Pseudo-Residuals**\n",
    "   For iteration $m$, compute the negative gradient of the loss wrt predictions:\n",
    "\n",
    "   $$\n",
    "   r_{im} = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x)=F_{m-1}(x)}\n",
    "   $$\n",
    "\n",
    "   These $r_{im}$ are the \"errors\" we want to fix.\n",
    "\n",
    "3. **Fit Weak Learner**\n",
    "   Train a weak model $h_m(x)$ (usually a shallow decision tree) to predict these residuals.\n",
    "\n",
    "4. **Update Model**\n",
    "   Update the ensemble prediction:\n",
    "\n",
    "   $$\n",
    "   F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)\n",
    "   $$\n",
    "\n",
    "   where $\\nu$ is the **learning rate** (step size, like in gradient descent).\n",
    "\n",
    "5. **Repeat**\n",
    "   Continue until convergence (or max trees).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Math Example (Regression with MSE)\n",
    "\n",
    "* Loss:\n",
    "\n",
    "  $$\n",
    "  L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2\n",
    "  $$\n",
    "\n",
    "* Gradient wrt prediction:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\n",
    "  $$\n",
    "\n",
    "* Negative gradient (pseudo-residual):\n",
    "\n",
    "  $$\n",
    "  r = y - \\hat{y}\n",
    "  $$\n",
    "\n",
    "üëâ So each new tree is simply trained to predict the **residuals**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. Connection to Gradient Descent\n",
    "\n",
    "* In **classical gradient descent**, we directly adjust weights:\n",
    "\n",
    "  $$\n",
    "  \\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)\n",
    "  $$\n",
    "* In **gradient boosting**, we adjust the **function (the model)** itself by adding trees:\n",
    "\n",
    "  $$\n",
    "  F_m(x) = F_{m-1}(x) - \\alpha \\nabla J(F_{m-1}(x))\n",
    "  $$\n",
    "\n",
    "  (but instead of moving directly, we fit a new weak learner to approximate that gradient).\n",
    "\n",
    "So:\n",
    "üëâ **Gradient Boosting = Gradient Descent in Function Space.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5. Key Hyperparameters in Gradient Boosting\n",
    "\n",
    "* **Number of Trees**: more trees ‚Üí better fit (but risk of overfitting).\n",
    "* **Learning Rate (ŒΩ)**: smaller = slower but more accurate.\n",
    "* **Tree Depth**: controls complexity of weak learners.\n",
    "* **Subsampling**: random sampling of data at each iteration (used in Stochastic Gradient Boosting).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 6. Famous Implementations\n",
    "\n",
    "* **GBM (Gradient Boosting Machine)**\n",
    "* **XGBoost** (extreme GBM, optimized for speed/regularization)\n",
    "* **LightGBM** (uses leaf-wise growth, very fast on large data)\n",
    "* **CatBoost** (handles categorical features efficiently).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary**\n",
    "\n",
    "* Gradient Boosting combines **boosting** + **gradient descent**.\n",
    "* Instead of updating weights directly, it adds weak learners trained on **gradients (residuals)** of the loss.\n",
    "* Think of it as ‚Äúbuilding a model by taking gradient descent steps in model space.‚Äù\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c85933",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
