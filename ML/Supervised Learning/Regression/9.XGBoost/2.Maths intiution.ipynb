{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e942ecd",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Objective Function\n",
    "\n",
    "At iteration $t$, the model prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "where $f_t$ is the new decision tree to be added.\n",
    "\n",
    "The overall objective is:\n",
    "\n",
    "$$\n",
    "Obj^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t)}) + \\sum_{k=1}^t \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "* $l$: loss function (e.g., log loss, MSE)\n",
    "* $\\Omega(f)$: regularization term controlling complexity of trees\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $T$: number of leaves\n",
    "* $w_j$: weight of leaf $j$\n",
    "* $\\gamma, \\lambda$: regularization parameters\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Second-Order Taylor Expansion\n",
    "\n",
    "Expand the loss around previous prediction $\\hat{y}_i^{(t-1)}$:\n",
    "\n",
    "$$\n",
    "l(y_i, \\hat{y}_i^{(t)}) \\approx l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "g_i = \\frac{\\partial l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i}, \\quad\n",
    "h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i)}{\\partial \\hat{y}_i^2}\n",
    "$$\n",
    "\n",
    "Thus, optimization depends only on **gradients (first derivative)** and **Hessians (second derivative)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Simplified Objective at Iteration $t$\n",
    "\n",
    "$$\n",
    "Obj^{(t)} \\approx \\sum_{j=1}^T \\left[ G_j w_j + \\frac{1}{2}(H_j + \\lambda) w_j^2 \\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "where for leaf $j$:\n",
    "\n",
    "* $G_j = \\sum_{i \\in I_j} g_i$\n",
    "* $H_j = \\sum_{i \\in I_j} h_i$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Optimal Leaf Weight\n",
    "\n",
    "For each leaf, optimal weight is:\n",
    "\n",
    "$$\n",
    "w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Optimal Value of a Tree\n",
    "\n",
    "The score of a tree (before adding) is:\n",
    "\n",
    "$$\n",
    "Obj^{(t)} = -\\frac{1}{2} \\sum_{j=1}^T \\frac{G_j^2}{H_j + \\lambda} + \\gamma T\n",
    "$$\n",
    "\n",
    "This tells us whether splitting a node improves the objective.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Split Gain (Decision Rule)\n",
    "\n",
    "When splitting a node into left (L) and right (R):\n",
    "\n",
    "$$\n",
    "Gain = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right) - \\gamma\n",
    "$$\n",
    "\n",
    "If **Gain > 0**, the split is useful.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition in Words\n",
    "\n",
    "* Each step builds a tree that minimizes the **second-order approximation** of the loss.\n",
    "* Gradients push predictions toward the correct direction.\n",
    "* Hessians adjust the step size for stability.\n",
    "* Regularization ($\\lambda, \\gamma$) penalizes complex trees, improving generalization.\n",
    "* Final model is the sum of many such optimized trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98265e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
