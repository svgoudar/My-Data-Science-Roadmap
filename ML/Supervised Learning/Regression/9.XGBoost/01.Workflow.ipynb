{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe9a618",
   "metadata": {},
   "source": [
    "Transcript explains **XGBoost (Extreme Gradient Boosting)** in detail through a step-by-step example.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Introduction**\n",
    "\n",
    "   * XGBoost handles classification and regression.\n",
    "   * Uses sequential decision trees to improve predictions.\n",
    "\n",
    "2. **Example Setup**\n",
    "\n",
    "   * Dataset: `salary` + `credit score` → `credit card approval (yes/no)`.\n",
    "   * Binary classification task.\n",
    "\n",
    "3. **Step 1: Base Model**\n",
    "\n",
    "   * Start with a base model giving constant probability (0.5).\n",
    "   * Calculate **residuals** = `actual − prediction`.\n",
    "\n",
    "4. **Step 2: First Decision Tree**\n",
    "\n",
    "   * Use residuals as targets to build a tree.\n",
    "   * Split features (`salary` or `credit`) based on **similarity score** and **gain**:\n",
    "\n",
    "     * **Similarity score** = Σ(residual²) ÷ Σ(p × (1−p)).\n",
    "     * **Gain** = (similarity left + similarity right) − similarity root.\n",
    "   * Select split with highest gain.\n",
    "\n",
    "5. **Tree Splitting Example**\n",
    "\n",
    "   * Feature chosen: `salary ≤ 50K` vs. `> 50K`.\n",
    "   * Residual values distributed across nodes.\n",
    "   * Compute similarity for left, right, and root.\n",
    "   * Gain guides which split is better.\n",
    "\n",
    "6. **Further Splits**\n",
    "\n",
    "   * Next feature `credit` is tested.\n",
    "   * Multiple splitting scenarios considered (bad/good/normal).\n",
    "   * Compute similarity and gain again.\n",
    "   * Continue splitting until stopping criteria met.\n",
    "\n",
    "7. **Prediction Process**\n",
    "\n",
    "   * For a new record:\n",
    "\n",
    "     * Base model output → transformed to **log odds**: log(p/(1−p)).\n",
    "     * Pass through decision trees → add weighted contributions.\n",
    "     * Apply **sigmoid activation** to convert back to probability.\n",
    "   * Example shown: base model (0.5 → log odds=0), learning rate α=0.1, similarity scores multiplied by α, final probability via sigmoid (\\~0.52 or 0.508 depending on record).\n",
    "\n",
    "8. **Sequential Model Construction**\n",
    "\n",
    "   * Final prediction = sigmoid( base learner + α₁·tree₁ + α₂·tree₂ + ... ).\n",
    "   * Each new tree trained on residuals of previous step.\n",
    "\n",
    "9. **Important Parameters**\n",
    "\n",
    "   * **Learning rate (α):** Controls contribution of each tree, prevents overfitting.\n",
    "   * **Lambda (λ):** Regularization hyperparameter included in similarity calculation (chosen via cross-validation).\n",
    "   * **Cover value:** Threshold based on p(1−p). If node weight < cover, stop splitting.\n",
    "\n",
    "10. **Extension**\n",
    "\n",
    "    * For regression: Similar process, formulas for similarity/gain differ slightly.\n",
    "    * For multiclass classification: sigmoid replaced by softmax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb93ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
