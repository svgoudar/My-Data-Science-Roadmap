{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fcda10",
   "metadata": {},
   "source": [
    "# ✅ Assumptions in Random Forest\n",
    "\n",
    "### 1. **Independence of Trees (via Randomness)**\n",
    "\n",
    "* Assumes each tree is trained on a **different bootstrap sample** of the data.\n",
    "* Feature randomness (choosing a random subset of features at each split) ensures trees are **decorrelated**.\n",
    "* The assumption: when averaged, uncorrelated (or weakly correlated) trees reduce variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Strong Learners from Weak Learners**\n",
    "\n",
    "* Each decision tree is a high-variance, low-bias model.\n",
    "* Random Forest assumes that combining many such high-variance learners through bagging leads to a low-variance, strong learner.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Sufficient Diversity in Data**\n",
    "\n",
    "* Assumes there’s enough variation in the training dataset such that different bootstrap samples + random feature subsets lead to diverse decision trees.\n",
    "* If the data is too homogeneous or too small, Random Forest won’t create enough diversity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Law of Large Numbers (Averaging Effect)**\n",
    "\n",
    "* Assumes that as the number of trees increases ($n \\to \\infty$), the average prediction of the forest converges to a stable and accurate result.\n",
    "* This is why Random Forests are generally not prone to overfitting as tree count grows (though training cost rises).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **No Strong Distributional Assumptions**\n",
    "\n",
    "* Unlike linear regression (needs linearity, homoscedasticity, normal residuals), Random Forest doesn’t assume:\n",
    "\n",
    "  * Linearity between features and target.\n",
    "  * Normal distribution of features or errors.\n",
    "  * Equal variance of residuals.\n",
    "\n",
    "This flexibility is why Random Forest works well on messy, real-world datasets.\n",
    "\n",
    "---\n",
    "\n",
    "# ⚖️ Summary\n",
    "\n",
    "Random Forest mainly assumes:\n",
    "\n",
    "1. Trees are diverse (via bootstrapping + random feature selection).\n",
    "2. Enough data exists to generate variation.\n",
    "3. Averaging many uncorrelated learners reduces variance → stronger generalization.\n",
    "\n",
    "It does **not** assume linearity, normality, or independence of features like parametric models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56bee8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
