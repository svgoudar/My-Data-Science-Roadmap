{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16809c97",
   "metadata": {},
   "source": [
    "# üå≥ Cost Function in Random Forest\n",
    "\n",
    "Random Forests don‚Äôt have a single global cost function like linear regression. Instead:\n",
    "\n",
    "### 1. **At Tree Level (Split Criterion)**\n",
    "\n",
    "* **Classification:**\n",
    "  Splits are chosen to minimize **impurity**.\n",
    "\n",
    "  * **Gini Impurity:**\n",
    "\n",
    "    $$\n",
    "    Gini(S) = 1 - \\sum_{k=1}^K p_k^2\n",
    "    $$\n",
    "  * **Entropy (Information Gain):**\n",
    "\n",
    "    $$\n",
    "    H(S) = -\\sum_{k=1}^K p_k \\log(p_k)\n",
    "    $$\n",
    "\n",
    "* **Regression:**\n",
    "  Splits minimize **variance (MSE)** of target values:\n",
    "\n",
    "  $$\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **At Forest Level**\n",
    "\n",
    "* Final prediction = **aggregate of trees**:\n",
    "\n",
    "  * Regression ‚Üí average of predictions.\n",
    "  * Classification ‚Üí majority vote (or average of predicted probabilities).\n",
    "\n",
    "* Cost function depends on task:\n",
    "\n",
    "  * **Regression Random Forest:**\n",
    "\n",
    "    $$\n",
    "    J_{RF} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i^{RF})^2\n",
    "    $$\n",
    "  * **Classification Random Forest (log loss):**\n",
    "\n",
    "    $$\n",
    "    J_{RF} = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^K y_{i,k} \\log(\\hat{p}_{i,k}^{RF})\n",
    "    $$\n",
    "\n",
    "‚úÖ Takeaway:\n",
    "\n",
    "* Trees minimize **impurity** locally.\n",
    "* The forest is evaluated by global error metrics like **MSE, accuracy, log-loss**.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è Hyperparameter Tuning in Random Forest\n",
    "\n",
    "Random Forests have several **hyperparameters** that control bias, variance, and speed:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Tree-related parameters**\n",
    "\n",
    "* `max_depth`: Maximum depth of each tree.\n",
    "\n",
    "  * Shallow ‚Üí high bias, low variance.\n",
    "  * Deep ‚Üí low bias, high variance.\n",
    "* `min_samples_split`: Minimum samples required to split a node.\n",
    "* `min_samples_leaf`: Minimum samples required at a leaf.\n",
    "* `max_features`: Number of features to consider when splitting.\n",
    "\n",
    "  * Common: `\"sqrt\"` (classification), `\"log2\"`, or a fraction.\n",
    "  * Controls correlation between trees (lower = more diverse trees).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Forest-related parameters**\n",
    "\n",
    "* `n_estimators`: Number of trees in the forest.\n",
    "\n",
    "  * More trees ‚Üí lower variance, higher computation.\n",
    "* `bootstrap`: Whether to use bootstrap sampling (default=True).\n",
    "* `max_samples`: Fraction of data sampled for each tree (if bootstrap=True).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Regularization parameters**\n",
    "\n",
    "* `max_leaf_nodes`: Maximum number of leaf nodes.\n",
    "* `ccp_alpha`: Complexity pruning parameter (post-pruning).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **How to Tune**\n",
    "\n",
    "* Use **Grid Search** or **Random Search** with **Cross-Validation**.\n",
    "* Common strategy:\n",
    "\n",
    "  1. Start with `n_estimators` high enough (e.g., 200+).\n",
    "  2. Tune `max_depth`, `min_samples_split`, `min_samples_leaf`.\n",
    "  3. Adjust `max_features` to control correlation between trees.\n",
    "  4. Use **Out-of-Bag (OOB) error** to evaluate instead of cross-validation (faster).\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Example (Scikit-Learn)\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Model\n",
    "rf = RandomForestClassifier(oob_score=True, random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.5]\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "search = RandomizedSearchCV(rf, param_distributions=param_grid, \n",
    "                            n_iter=20, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", search.best_params_)\n",
    "print(\"OOB Score:\", search.best_estimator_.oob_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary:**\n",
    "\n",
    "* **Cost function** in Random Forest = impurity measures (local) + global evaluation (MSE/log-loss).\n",
    "* **Hyperparameters** control tree depth, number of trees, and randomness.\n",
    "* **Tuning** = balance bias vs. variance for best generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e655dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
