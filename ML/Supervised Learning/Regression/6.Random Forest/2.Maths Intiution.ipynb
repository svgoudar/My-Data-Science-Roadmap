{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192f9f2a",
   "metadata": {},
   "source": [
    "# ðŸ”¢ Mathematical Intuition of Random Forest\n",
    "\n",
    "### 1. Prediction from a single decision tree\n",
    "\n",
    "Letâ€™s denote:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t(x)\n",
    "$$\n",
    "\n",
    "\\= prediction from the $t^{th}$ tree for input $x$.\n",
    "\n",
    "For **regression**, a single treeâ€™s prediction is just:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t(x) = \\frac{1}{N_t} \\sum_{i \\in L_t(x)} y_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $L_t(x)$ = set of training samples in the leaf node where $x$ falls in tree $t$.\n",
    "* $N_t$ = number of samples in that leaf.\n",
    "\n",
    "So each tree is basically computing a **local average** of target values.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Random Forest prediction (averaging trees)\n",
    "\n",
    "For **regression**:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{RF}(x) = \\frac{1}{T} \\sum_{t=1}^T \\hat{y}_t(x)\n",
    "$$\n",
    "\n",
    "For **classification**:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{RF}(x) = \\arg\\max_{c} \\sum_{t=1}^T \\mathbb{1}\\big(\\hat{y}_t(x) = c\\big)\n",
    "$$\n",
    "\n",
    "(i.e., majority vote among trees).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Variance reduction (key intuition)\n",
    "\n",
    "If we assume:\n",
    "\n",
    "* Each tree is an estimator with variance $\\sigma^2$.\n",
    "* Correlation between two trees = $\\rho$.\n",
    "\n",
    "Then, variance of the **average of T trees** is:\n",
    "\n",
    "$$\n",
    "Var(\\hat{y}_{RF}(x)) = \\rho \\sigma^2 + \\frac{1-\\rho}{T} \\sigma^2\n",
    "$$\n",
    "\n",
    "ðŸ‘‰ Insights:\n",
    "\n",
    "* If trees are **independent** ($\\rho \\approx 0$):\n",
    "\n",
    "  $$\n",
    "  Var \\approx \\frac{\\sigma^2}{T}\n",
    "  $$\n",
    "\n",
    "  â†’ averaging reduces variance dramatically.\n",
    "* If trees are **highly correlated** ($\\rho \\approx 1$):\n",
    "\n",
    "  $$\n",
    "  Var \\approx \\sigma^2\n",
    "  $$\n",
    "\n",
    "  â†’ no benefit from averaging.\n",
    "\n",
    "Thatâ€™s why Random Forest injects **randomness** (bootstrap samples + random feature subsets) to **reduce correlation** between trees.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Bias-Variance Tradeoff\n",
    "\n",
    "* A single tree has **low bias** (can fit complex patterns) but **high variance**.\n",
    "* Random Forest keeps bias roughly the same but reduces variance by averaging.\n",
    "* Mathematically:\n",
    "\n",
    "$$\n",
    "Error = Bias^2 + Variance + Irreducible\\ Noise\n",
    "$$\n",
    "\n",
    "Random Forest â†’ keeps $Bias^2$ low, shrinks **Variance**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Key Takeaway\n",
    "\n",
    "The math behind Random Forest is really about:\n",
    "\n",
    "1. Each tree = local average prediction.\n",
    "2. Forest = average of many trees.\n",
    "3. Averaging â†’ reduces variance, stability improves.\n",
    "4. Randomness â†’ reduces correlation between trees â†’ more effective averaging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a1449",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
