{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bd1c2",
   "metadata": {},
   "source": [
    "## **AdaBoost (Adaptive Boosting) – Overview**\n",
    "\n",
    "**AdaBoost** is a popular **ensemble learning algorithm** used primarily for **classification** (but can also be adapted for regression). Its core idea is to **combine multiple “weak learners” into a strong learner**.\n",
    "\n",
    "* **Weak Learner:** A model slightly better than random guessing (e.g., decision stump – a single-level decision tree).\n",
    "* **Strong Learner:** A combination of weak learners that achieves high accuracy.\n",
    "\n",
    "AdaBoost adapts by **focusing more on the misclassified instances** from previous learners in each iteration.\n",
    "\n",
    "---\n",
    "\n",
    "## **Core Steps of AdaBoost**\n",
    "\n",
    "1. **Initialize Sample Weights**\n",
    "\n",
    "   * Start with equal weights for all training samples:\n",
    "\n",
    "   $$\n",
    "   w_i = \\frac{1}{N}, \\quad i = 1,2,...,N\n",
    "   $$\n",
    "\n",
    "   where $N$ is the total number of training samples.\n",
    "\n",
    "2. **Train Weak Learner**\n",
    "\n",
    "   * Fit a weak learner (e.g., a decision stump) using the **weighted dataset**.\n",
    "\n",
    "3. **Compute Learner Error**\n",
    "\n",
    "   * Calculate the weighted error of the weak learner:\n",
    "\n",
    "   $$\n",
    "   \\text{error}_t = \\frac{\\sum_{i=1}^{N} w_i \\cdot \\mathbf{1}(y_i \\neq h_t(x_i))}{\\sum_{i=1}^{N} w_i}\n",
    "   $$\n",
    "\n",
    "   where $h_t(x_i)$ is the prediction of the t-th weak learner, and $\\mathbf{1}$ is the indicator function (1 if wrong, 0 if correct).\n",
    "\n",
    "4. **Compute Learner Weight (Alpha)**\n",
    "\n",
    "   * Assign a weight to the weak learner based on its accuracy:\n",
    "\n",
    "   $$\n",
    "   \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\text{error}_t}{\\text{error}_t}\\right)\n",
    "   $$\n",
    "\n",
    "   * Better learners get higher weights.\n",
    "\n",
    "5. **Update Sample Weights**\n",
    "\n",
    "   * Increase weights of misclassified samples so the next learner focuses on them:\n",
    "\n",
    "   $$\n",
    "   w_i \\leftarrow w_i \\cdot e^{\\alpha_t \\cdot \\mathbf{1}(y_i \\neq h_t(x_i))}\n",
    "   $$\n",
    "\n",
    "   * Normalize weights so they sum to 1.\n",
    "\n",
    "6. **Repeat**\n",
    "\n",
    "   * Steps 2–5 are repeated for $T$ weak learners.\n",
    "\n",
    "7. **Final Prediction**\n",
    "\n",
    "   * The strong classifier is a **weighted majority vote** of all weak learners:\n",
    "\n",
    "   $$\n",
    "   H(x) = \\text{sign} \\Bigg( \\sum_{t=1}^{T} \\alpha_t \\cdot h_t(x) \\Bigg)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Intuition**\n",
    "\n",
    "* AdaBoost **focuses on hard-to-classify samples** by increasing their weights.\n",
    "* Weak learners are combined using **weighted voting**; more accurate learners have more influence.\n",
    "* Despite using weak learners individually, the ensemble often achieves **high accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages**\n",
    "\n",
    "* Works well with small weak learners.\n",
    "* Less prone to overfitting compared to single deep trees.\n",
    "* Adaptive: focuses on hard samples.\n",
    "\n",
    "## **Disadvantages**\n",
    "\n",
    "* Sensitive to **noisy data** and **outliers**.\n",
    "* Can be computationally expensive if many iterations are used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d195e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
