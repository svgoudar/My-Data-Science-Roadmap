{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564b01b7",
   "metadata": {},
   "source": [
    "\n",
    "### **Key Assumptions in AdaBoost**\n",
    "\n",
    "1. **Weak Learners Should Perform Slightly Better than Random**\n",
    "\n",
    "   * Each base learner (weak classifier) must have an accuracy **better than 50%** for binary classification.\n",
    "   * If the weak learner is worse than random guessing, AdaBoost cannot improve performance effectively.\n",
    "\n",
    "2. **Focus on Misclassified Samples**\n",
    "\n",
    "   * AdaBoost assumes that **errors of previous learners can be corrected** by focusing more on misclassified data points in subsequent iterations.\n",
    "   * The algorithm adapts by giving **higher weights to misclassified samples** for the next weak learner.\n",
    "\n",
    "3. **Independent Errors of Weak Learners**\n",
    "\n",
    "   * AdaBoost works best if the errors of weak learners are **uncorrelated**, i.e., each weak learner makes mistakes on different parts of the data.\n",
    "   * Highly correlated errors may reduce the overall performance gain.\n",
    "\n",
    "4. **Binary or Properly Encoded Targets**\n",
    "\n",
    "   * The standard AdaBoost assumes **binary classification**, usually encoded as $\\{-1, +1\\}$. Multi-class extensions exist but rely on proper encoding.\n",
    "\n",
    "5. **Noisy Data Can Affect Performance**\n",
    "\n",
    "   * AdaBoost assumes that the training data is **relatively clean**. Outliers or mislabeled points can get **high weights** and dominate the learning, potentially harming performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d41fc9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
