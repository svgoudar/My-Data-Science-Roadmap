{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e942ecd",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Adaboost revolves around an **iterative process of adjusting data point weights and weak learner contributions** to progressively improve classification accuracy. Adaboost aims to combine **multiple \"weak learners\"**—primarily **Decision Tree Stumps (DTS)**, which are decision trees of depth 1—into a **\"strong learner\"**.\n",
    "\n",
    "Here are the mathematical steps involved:\n",
    "\n",
    "1.  **Initial Sample Weights**:\n",
    "    *   At the beginning of the process, **all data points are assigned an equal sample weight**. For instance, if there are 7 data points, each would initially have a weight of 1/7.\n",
    "\n",
    "2.  **Selecting the Best Decision Tree Stump (Weak Learner)**:\n",
    "    *   In each iteration, Adaboost **creates and selects the \"best\" Decision Tree Stump**. This selection is based on criteria like **Entropy or Gini Impurity**, aiming to find the stump that best separates the data. This selected stump is the first weak learner, often denoted as `M1`.\n",
    "\n",
    "3.  **Calculating Total Error (TE)**:\n",
    "    *   After a DTS is selected, its **Total Error (TE)** is calculated. The Total Error represents the **proportion of misclassified data points** by that specific stump. For example, if 1 out of 7 data points is misclassified by the DTS, the Total Error (`TE`) would be 1/7.\n",
    "\n",
    "4.  **Calculating the Performance of the Stump (Weight of the Weak Learner)**:\n",
    "    *   A crucial step is to calculate the \"Performance of Stump,\" denoted by `α` (alpha), which represents the **weight or contribution of the current weak learner** to the final model. This `α` is calculated using the formula:\n",
    "        **`α = (1/2) * ln[(1 - TE) / TE]`**\n",
    "    *   A higher `α` value indicates a better-performing stump. For example, if `TE = 1/7`, then `α1` would be approximately 0.896. This `α` value determines how much influence this specific weak learner (`M`) will have in the final Adaboost prediction function `f = α1(M1) + α2(M2) + ... + αn(Mn)`.\n",
    "\n",
    "5.  **Updating Weights for Data Points**:\n",
    "    *   After calculating the `α` for the current stump, the **weights of individual data points are updated** to focus on previously misclassified examples.\n",
    "        *   For **correctly classified points**, their weights are **decreased** by multiplying their old weight by `e^(-Performance of Stump)` (i.e., `e^(-α)`). This makes them less important for the subsequent iterations.\n",
    "        *   For **incorrectly classified points**, their weights are **increased** by multiplying their old weight by `e^(Performance of Stump)` (i.e., `e^(α)`). This makes misclassified points more influential for the next weak learner. For instance, a misclassified point might have its weight updated from 1/7 to 0.349.\n",
    "\n",
    "6.  **Normalizing Weights and Assigning Bins**:\n",
    "    *   The updated weights for all data points are then **normalized** so that their sum approximately equals 1.\n",
    "    *   These normalized weights are used to create \"Bins Assignment\". Each data point is assigned a range (or \"bin\") within a 0-1 scale that is **proportional to its normalized weight**. Crucially, incorrectly classified points, having higher normalized weights, will be assigned **larger bins**. For example, a correctly classified point might have a bin from 0-0.08, while an incorrectly classified one might have a bin from 0.40-0.70.\n",
    "\n",
    "7.  **Selecting Data Points for the Next Stump**:\n",
    "    *   This is the mechanism for ensuring that subsequent weak learners focus on the difficult examples. For each record, a **random value between 0 and 1 is generated**. If this random value falls within a data point's assigned bin, that data point is **selected** to be sent to the next Decision Tree Stump (e.g., `DTS2`). Because misclassified points have larger bins, they have a higher probability of being selected, forcing the next weak learner to address them. This iterative process of selecting data, building a stump, calculating error and performance, and updating weights continues for multiple iterations.\n",
    "\n",
    "8.  **Final Prediction**:\n",
    "    *   After training multiple weak learners (DTS1, DTS2, ..., DTSN) through this iterative process, Adaboost makes a final prediction. For a given test data point, each individual DTS makes its prediction (e.g., Yes/No).\n",
    "    *   The final prediction is then a **weighted sum of the predictions from all the weak learners**, where each weak learner's prediction (`M`) is weighted by its calculated performance score (`α`). The formula for the final prediction `f` is:\n",
    "        **`f = α1(M1) + α2(M2) + α3(M3) + ... + αn(Mn)`**\n",
    "    *   The final output class is determined by **summing the performance scores for each class** (e.g., 'Yes' or 'No'). The class with the higher aggregated performance score becomes the final prediction. For example, if the total weighted score for \"Yes\" is 1.136 and for \"No\" is 0.350, the final output would be \"Yes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98265e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
