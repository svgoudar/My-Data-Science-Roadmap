{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjXjqRcsCZ66crwTTd+X0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svgoudar/My-Data-Science-Roadmap/blob/main/ML/Supervised%20Learning/Regression/Linear%20Regression/6.Reggression%20with%20OLS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a clear and focused summary of the **GeeksforGeeks** article on **using Ordinary Least Squares (OLS) with the `statsmodels` library** in Python:\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        " **OLS regression** is a fundamental statistical method for estimating the parameters of a **linear regression model**. It emphasizes how OLS works by **minimizing the sum of squared residuals** (the vertical distances between observed and predicted values).\n",
        "\n",
        "---\n",
        "\n",
        "## Key Highlights\n",
        "\n",
        "1. **Linear Regression Basics**\n",
        "\n",
        "   * OLS models the relationship between a dependent variable $\\hat{y}$ and one or more independent variables $x$:\n",
        "\n",
        "     $$\n",
        "     \\hat{y} = b_0 + b_1 x + \\dots\n",
        "     $$\n",
        "   * It aims to minimize:\n",
        "\n",
        "     $$\n",
        "     S = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "     $$\n",
        "\n",
        "     where $b_0$ is the intercept and $b_1$ the slope or coefficient.\n",
        "     ([GeeksforGeeks][1])\n",
        "\n",
        "2. **Using `statsmodels` for OLS**\n",
        "\n",
        "   * You set up your model using `sm.OLS()`, fit it, and then call `.summary()` to view detailed outputs.\n",
        "   * This includes coefficients, statistical significance (p-values), R-squared, and other performance diagnostics.\n",
        "     ([GeeksforGeeks][2])\n",
        "\n",
        "3. **Interpreting the Summary**\n",
        "\n",
        "   * The summary provides essential model metrics:\n",
        "\n",
        "     * **Coefficients**: estimated intercept and slopes.\n",
        "     * **R-squared / Adjusted R-squared**: how well the model explains target variance.\n",
        "     * **p-values**: statistical significance tests for each predictor.\n",
        "     * **F-statistic**: overall model significance.\n",
        "     * **Standard Errors**: measure of estimation precision.\n",
        "       ([GeeksforGeeks][2])\n",
        "\n",
        "4. **Why It Matters**\n",
        "\n",
        "   * OLS in `statsmodels` is not only easy to implement, but also provides deep statistical insights through hypothesis testing and diagnostics, making it ideal for exploratory analysis and model evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Step                  | Action                                    |\n",
        "| --------------------- | ----------------------------------------- |\n",
        "| Define model          | Use `sm.OLS()` with predictors and target |\n",
        "| Fit model             | `.fit()` method                           |\n",
        "| Evaluate output       | `.summary()` gives detailed metrics       |\n",
        "| Interpret key results | Coefficients, R-squared, p-values, etc.   |\n"
      ],
      "metadata": {
        "id": "ZDJoQ82elTMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# 1️⃣ Load dataset\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "df = data.frame\n",
        "\n",
        "# Let's pick some features\n",
        "X = df[[\"MedInc\", \"AveRooms\", \"AveOccup\"]]  # Independent variables\n",
        "y = df[\"MedHouseVal\"]  # Dependent variable\n",
        "\n",
        "# 2️⃣ Add a constant (intercept) for OLS\n",
        "X = sm.add_constant(X)  # Adds column of ones for b0\n",
        "\n",
        "# 3️⃣ Create and fit OLS model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# 4️⃣ View summary\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7Z-NzOYle0s",
        "outputId": "57252e5a-1823-4996-d0e6-3441a10247ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:            MedHouseVal   R-squared:                       0.481\n",
            "Model:                            OLS   Adj. R-squared:                  0.481\n",
            "Method:                 Least Squares   F-statistic:                     6370.\n",
            "Date:                Sun, 10 Aug 2025   Prob (F-statistic):               0.00\n",
            "Time:                        15:26:04   Log-Likelihood:                -25477.\n",
            "No. Observations:               20640   AIC:                         5.096e+04\n",
            "Df Residuals:                   20636   BIC:                         5.099e+04\n",
            "Df Model:                           3                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.6069      0.016     37.444      0.000       0.575       0.639\n",
            "MedInc         0.4347      0.003    134.806      0.000       0.428       0.441\n",
            "AveRooms      -0.0383      0.002    -15.482      0.000      -0.043      -0.033\n",
            "AveOccup      -0.0042      0.001     -7.488      0.000      -0.005      -0.003\n",
            "==============================================================================\n",
            "Omnibus:                     4836.746   Durbin-Watson:                   0.693\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12992.753\n",
            "Skew:                           1.256   Prob(JB):                         0.00\n",
            "Kurtosis:                       5.965   Cond. No.                         31.4\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an explanation of each field in the `OLS.summary()` output, using your provided example for context.\n",
        "\n",
        "### Model Summary\n",
        "This section gives you a high-level view of how well the overall model performs.\n",
        "\n",
        "* **Dep. Variable: MedHouseVal**: This is the dependent variable you're trying to predict—in this case, the median house value.\n",
        "* **R-squared ($R^2$): 0.481**: This indicates that approximately **48.1%** of the variation in `MedHouseVal` can be explained by the independent variables in your model (`MedInc`, `AveRooms`, and `AveOccup`).\n",
        "* **Adj. R-squared ($R^2_{adj}$): 0.481**: This is a modified $R^2$ that accounts for the number of predictors. Since it's very close to the regular $R^2$, it suggests that all the predictors in the model are likely useful.\n",
        "* **F-statistic: 6370.**: This is a test for the overall significance of the model. The large value suggests that at least one of your independent variables is significantly related to the dependent variable.\n",
        "* **Prob (F-statistic): 0.00**: The p-value for the F-statistic. A value of 0.00 (which is less than 0.05) indicates that the model is statistically significant.\n",
        "* **No. Observations: 20640**: This is the number of data points used to train the model.\n",
        "\n",
        "---\n",
        "\n",
        "### Coefficients Table\n",
        "This table details the individual impact of each predictor on the dependent variable.\n",
        "\n",
        "* **const (coefficient): 0.6069**: This is the y-intercept. It's the predicted value of `MedHouseVal` when all the independent variables are zero.\n",
        "* **MedInc (coefficient): 0.4347**: For every one-unit increase in the median income, the median house value is predicted to increase by **0.4347**, holding all other variables constant.\n",
        "* **AveRooms (coefficient): -0.0383**: For every one-unit increase in the average number of rooms, the median house value is predicted to decrease by **0.0383**, holding all other variables constant.\n",
        "* **AveOccup (coefficient): -0.0042**: For every one-unit increase in the average number of occupants, the median house value is predicted to decrease by **0.0042**, holding all other variables constant.\n",
        "* **t-statistic**: For each predictor, this value helps determine if the coefficient is statistically different from zero. For example, `MedInc` has a very high t-statistic (**134.806**), which is strong evidence that its coefficient is significant.\n",
        "* **P > |t|**: The p-value for each coefficient. All of your predictors have a p-value of **0.000**, which is much less than the typical significance level of 0.05. This means all three predictors are statistically significant in explaining `MedHouseVal`.\n",
        "* **[0.025, 0.975]**: This is the 95% confidence interval for each coefficient. For `MedInc`, the interval is `[0.428, 0.441]`. Since this range does not include zero, it reinforces that the relationship is statistically significant.\n",
        "\n",
        "---\n",
        "\n",
        "### Diagnostics Table\n",
        "This section helps you assess if the underlying assumptions of linear regression are met.\n",
        "\n",
        "* **Omnibus: 4836.746**: This tests the normality of the residuals (the differences between the predicted and actual values). A large value suggests that the residuals are not normally distributed, which is an assumption of OLS.\n",
        "* **Prob(Omnibus): 0.000**: A low p-value here confirms that the residuals are not normally distributed.\n",
        "* **Durbin-Watson: 0.693**: This tests for autocorrelation (correlation between consecutive residuals). A value close to 2 indicates no autocorrelation. Your value of **0.693** suggests a high degree of positive autocorrelation, which may mean there are unobserved variables or that your model is missing some time-series components.\n",
        "* **Jarque-Bera (JB): 12992.753**: Another test for normality. A high value confirms the residuals are not normally distributed.\n",
        "* **Skew: 1.256**: This measures the asymmetry of the residuals. A positive skew indicates that the residuals are skewed to the right.\n",
        "* **Kurtosis: 5.965**: This measures the \"peakedness\" of the residuals. A value significantly higher than 3 (the kurtosis of a normal distribution) indicates that the residuals have \"fat tails,\" meaning there are more outliers than you'd expect in a normal distribution.\n",
        "* **Cond. No.: 31.4**: This checks for multicollinearity (when independent variables are highly correlated). A value above 30 indicates a problem. Your value of **31.4** is slightly above this threshold, suggesting some multicollinearity issues that might be affecting the stability of your coefficient estimates.\n",
        "\n",
        "This output indicates that your model is statistically significant and its predictors are useful. However, the diagnostics section reveals potential issues, such as non-normal residuals and positive autocorrelation, that you may need to address for a more robust model."
      ],
      "metadata": {
        "id": "tg_i7xb_CvFK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZ2jgDYyvwOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}