{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a0bdb02",
   "metadata": {},
   "source": [
    "## 🌳 **Assumptions in Decision Trees**\n",
    "\n",
    "### 1. **Features split data meaningfully**\n",
    "\n",
    "* Assumes that some features (or combinations) can split the data into groups that are closer to being “pure.”\n",
    "* Example: Outlook = *Overcast* always → Play Tennis = *Yes*.\n",
    "* If features don’t provide meaningful splits, the tree struggles.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Greedy splitting is “good enough”**\n",
    "\n",
    "* At each node, the tree chooses the *best split locally* (using Entropy, Gini, etc.).\n",
    "* Assumes that this greedy approach leads to a reasonably good global structure.\n",
    "* It does **not** check all possible tree structures (too expensive).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Independence between features (conditional on splits)**\n",
    "\n",
    "* Once a split is made, the tree assumes that subsequent splits don’t need to revisit earlier features.\n",
    "* Order of splits matters — once a feature is used, it won’t usually be reused above.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Data is sufficient to avoid overfitting**\n",
    "\n",
    "* Trees assume there’s enough training data to learn meaningful splits.\n",
    "* Small datasets → risk of capturing noise (overfitting).\n",
    "* That’s why pruning or ensemble methods (Random Forests, XGBoost) are used.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Splitting criteria reflect “purity” correctly**\n",
    "\n",
    "* Assumes measures like **Entropy**, **Gini Index**, or **Information Gain** truly capture which feature is best.\n",
    "* In skewed datasets, these measures can sometimes be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Stationarity of data**\n",
    "\n",
    "* Assumes the relationship between features and target remains consistent in train vs. test (no major distribution shift).\n",
    "* Otherwise, splits learned won’t generalize.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key takeaway**: Decision trees don’t assume linearity, normality, or homoscedasticity like regression models. Their main assumptions are: *features provide useful splits, greedy local decisions lead to a good global tree, and enough data exists to prevent overfitting*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394edbfb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
