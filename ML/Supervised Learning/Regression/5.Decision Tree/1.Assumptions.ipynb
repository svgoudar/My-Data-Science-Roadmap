{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a0bdb02",
   "metadata": {},
   "source": [
    "## ğŸŒ³ **Assumptions in Decision Trees**\n",
    "\n",
    "### 1. **Features split data meaningfully**\n",
    "\n",
    "* Assumes that some features (or combinations) can split the data into groups that are closer to being â€œpure.â€\n",
    "* Example: Outlook = *Overcast* always â†’ Play Tennis = *Yes*.\n",
    "* If features donâ€™t provide meaningful splits, the tree struggles.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Greedy splitting is â€œgood enoughâ€**\n",
    "\n",
    "* At each node, the tree chooses the *best split locally* (using Entropy, Gini, etc.).\n",
    "* Assumes that this greedy approach leads to a reasonably good global structure.\n",
    "* It does **not** check all possible tree structures (too expensive).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Independence between features (conditional on splits)**\n",
    "\n",
    "* Once a split is made, the tree assumes that subsequent splits donâ€™t need to revisit earlier features.\n",
    "* Order of splits matters â€” once a feature is used, it wonâ€™t usually be reused above.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Data is sufficient to avoid overfitting**\n",
    "\n",
    "* Trees assume thereâ€™s enough training data to learn meaningful splits.\n",
    "* Small datasets â†’ risk of capturing noise (overfitting).\n",
    "* Thatâ€™s why pruning or ensemble methods (Random Forests, XGBoost) are used.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Splitting criteria reflect â€œpurityâ€ correctly**\n",
    "\n",
    "* Assumes measures like **Entropy**, **Gini Index**, or **Information Gain** truly capture which feature is best.\n",
    "* In skewed datasets, these measures can sometimes be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Stationarity of data**\n",
    "\n",
    "* Assumes the relationship between features and target remains consistent in train vs. test (no major distribution shift).\n",
    "* Otherwise, splits learned wonâ€™t generalize.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Key takeaway**: Decision trees donâ€™t assume linearity, normality, or homoscedasticity like regression models. Their main assumptions are: *features provide useful splits, greedy local decisions lead to a good global tree, and enough data exists to prevent overfitting*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394edbfb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
