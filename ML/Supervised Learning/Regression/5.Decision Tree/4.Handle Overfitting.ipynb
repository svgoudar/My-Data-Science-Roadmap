{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e082affc",
   "metadata": {},
   "source": [
    "Hello everyone! In this video, we’ll discuss a very important topic: **Pre-pruning** and **Post-pruning** in Decision Trees.\n",
    "\n",
    "To begin, let’s understand the meaning of pruning. If you think of a gardener trimming plants to maintain their shape and promote healthy growth, pruning in decision trees works similarly. It helps improve the tree’s performance by avoiding unnecessary complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Suppose we have a training dataset and use a decision tree algorithm with its default parameters. The tree will keep splitting the data until it reaches the leaf nodes, ensuring all splits are pure.\n",
    "\n",
    "For instance:\n",
    "\n",
    "- At one point, the split might result in a leaf node with 9 \"Yes\" and 2 \"No\" categories. However, the algorithm may continue splitting further, creating nodes like:\n",
    "  - 9 \"Yes\" and 0 \"No\"\n",
    "  - 0 \"Yes\" and 2 \"No\"\n",
    "\n",
    "While these splits are pure, they are unnecessary and lead to a common problem: **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overfitting in Decision Trees\n",
    "\n",
    "Overfitting occurs when a decision tree learns the training data too well, including noise and irrelevant details. This results in:\n",
    "\n",
    "1. **High training accuracy** but\n",
    "2. **Low test accuracy**, making the model less generalizable.\n",
    "\n",
    "Overfitting can be characterized as:\n",
    "\n",
    "- **Low Bias** (good fit for training data)\n",
    "- **High Variance** (poor performance on test data)\n",
    "\n",
    "To address this issue, we use **pruning techniques**:\n",
    "\n",
    "- **Post-pruning**\n",
    "- **Pre-pruning**\n",
    "\n",
    "---\n",
    "\n",
    "### Post-pruning\n",
    "\n",
    "**Definition:** In post-pruning, we first construct the complete decision tree and then prune it by removing unnecessary branches.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- If a split results in 9 \"Yes\" and 2 \"No\" categories, the tree can stop further splitting and make the output \"Yes\" directly. This avoids creating additional pure splits, saving computation and reducing overfitting.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. **Construct the complete decision tree**\n",
    "2. **Prune the tree** by cutting branches based on certain criteria, such as depth or impurity thresholds.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Suitable for **smaller datasets**, as building the entire tree for large datasets can be computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "### Pre-pruning\n",
    "\n",
    "**Definition:** In pre-pruning, we prevent the tree from growing excessively by setting constraints during its construction. This involves tuning hyperparameters to limit the tree’s depth and complexity.\n",
    "\n",
    "#### Hyperparameters to Tune\n",
    "\n",
    "- **max_depth**: Maximum depth of the tree.\n",
    "- **max_features**: Maximum number of features to consider for a split.\n",
    "- **min_samples_split**: Minimum samples required to split a node.\n",
    "- **min_samples_leaf**: Minimum samples required to be at a leaf node.\n",
    "- **criterion**: The function to measure split quality (e.g., Gini, entropy).\n",
    "\n",
    "#### How to Perform Pre-pruning\n",
    "\n",
    "1. Use hyperparameter tuning techniques like **GridSearchCV** to find the optimal values for these parameters.\n",
    "2. Set these parameters before constructing the tree.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- More efficient for **large datasets**, as it avoids constructing the full tree.\n",
    "- Reduces time complexity by controlling tree growth during construction.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| **Aspect**      | **Post-pruning**                     | **Pre-pruning**                       |\n",
    "| --------------- | ------------------------------------ | ------------------------------------- |\n",
    "| **Process**     | Build the full tree, then prune it   | Prune the tree during construction    |\n",
    "| **Use Case**    | Small datasets                       | Large datasets                        |\n",
    "| **Approach**    | Remove unnecessary branches post hoc | Limit tree growth via hyperparameters |\n",
    "| **Computation** | Higher time complexity               | Lower time complexity                 |\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Demonstration\n",
    "\n",
    "Let’s quickly explore the relevant hyperparameters in the **scikit-learn DecisionTreeClassifier** documentation:\n",
    "\n",
    "1. **criterion**: Options include Gini, entropy, and log loss.\n",
    "2. **splitter**: Defines the strategy for choosing the split at each node (e.g., best or random).\n",
    "3. **max_depth**: Limits the depth of the tree.\n",
    "4. **min_samples_split**: Minimum samples required to split a node.\n",
    "5. **min_samples_leaf**: Minimum samples required at a leaf node.\n",
    "6. **max_features**: Number of features to consider for a split.\n",
    "\n",
    "These parameters allow fine-tuning to achieve optimal performance while avoiding overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "To reduce overfitting in decision trees, we can:\n",
    "\n",
    "1. Use **Post-pruning**: Construct the full tree and prune it afterward (best for small datasets).\n",
    "2. Use **Pre-pruning**: Limit tree growth during construction through hyperparameter tuning (best for large datasets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e63b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
