{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb19992",
   "metadata": {},
   "source": [
    "# üå≥ Cost Function of a Decision Tree\n",
    "\n",
    "A **Decision Tree** doesn‚Äôt optimize a single global cost function like linear regression.\n",
    "Instead, it makes **greedy, local decisions** at each split.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **At Each Split (Node-Level Cost Function)**\n",
    "\n",
    "The algorithm tries to find the \"best\" feature and threshold to split data.\n",
    "\"Best\" means **reducing impurity (classification)** or **reducing variance (regression).**\n",
    "\n",
    "### üîπ For Classification\n",
    "\n",
    "* **Entropy (Information Gain):**\n",
    "\n",
    "  $$\n",
    "  H(S) = -\\sum_{k=1}^K p_k \\log_2(p_k)\n",
    "  $$\n",
    "\n",
    "  * Where $p_k$ = proportion of class $k$ in node $S$.\n",
    "  * A pure node (all samples from one class) ‚Üí $H(S) = 0$.\n",
    "\n",
    "* **Gini Impurity:**\n",
    "\n",
    "  $$\n",
    "  Gini(S) = 1 - \\sum_{k=1}^K p_k^2\n",
    "  $$\n",
    "\n",
    "* **Information Gain (Cost Reduction):**\n",
    "\n",
    "  $$\n",
    "  IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "  $$\n",
    "\n",
    "  where $S_v$ is the subset after splitting on feature $A$.\n",
    "\n",
    "üëâ The split chosen is the one **maximizing Information Gain** (or equivalently, minimizing weighted impurity).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ For Regression\n",
    "\n",
    "* **Mean Squared Error (MSE):**\n",
    "\n",
    "  $$\n",
    "  MSE(S) = \\frac{1}{|S|} \\sum_{i \\in S} (y_i - \\bar{y}_S)^2\n",
    "  $$\n",
    "\n",
    "* **Variance Reduction (Cost Reduction):**\n",
    "\n",
    "  $$\n",
    "  \\Delta = Var(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Var(S_v)\n",
    "  $$\n",
    "\n",
    "üëâ The split chosen is the one that **minimizes variance (or MSE)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Global Cost Function (Tree-Level Evaluation)**\n",
    "\n",
    "At the end, the Decision Tree is evaluated with standard metrics:\n",
    "\n",
    "* Classification ‚Üí Accuracy, Log Loss, F1-score.\n",
    "* Regression ‚Üí MSE, MAE, RMSE, $R^2$.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è Hyperparameters in Decision Tree\n",
    "\n",
    "Decision Trees are **prone to overfitting**, so hyperparameters act like regularizers.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Tree Growth Parameters**\n",
    "\n",
    "* `max_depth`: Maximum depth of the tree.\n",
    "\n",
    "  * Shallow tree ‚Üí high bias, low variance.\n",
    "  * Deep tree ‚Üí low bias, high variance (overfits).\n",
    "\n",
    "* `min_samples_split`: Minimum samples needed to split a node.\n",
    "\n",
    "  * Prevents splitting on very small subsets.\n",
    "\n",
    "* `min_samples_leaf`: Minimum samples required at a leaf node.\n",
    "\n",
    "  * Larger values ‚Üí smoother predictions, less overfitting.\n",
    "\n",
    "* `max_leaf_nodes`: Maximum number of leaves in the tree.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Feature Selection Parameters**\n",
    "\n",
    "* `max_features`: Number of features considered for splitting at each node.\n",
    "\n",
    "  * If less than total features ‚Üí introduces randomness, prevents greedy fitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Splitting Parameters**\n",
    "\n",
    "* `criterion`: Cost function used for splits.\n",
    "\n",
    "  * `\"gini\"` or `\"entropy\"` for classification.\n",
    "  * `\"mse\"` or `\"friedman_mse\"` for regression.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Regularization Parameters**\n",
    "\n",
    "* `ccp_alpha`: Complexity pruning parameter (post-pruning).\n",
    "\n",
    "  * Higher `ccp_alpha` ‚Üí simpler tree (removes weak branches).\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Example in Scikit-Learn\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",       # or \"entropy\"\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary**\n",
    "\n",
    "* **Cost function (splitting criterion):**\n",
    "\n",
    "  * Classification ‚Üí minimize Gini or Entropy (maximize info gain).\n",
    "  * Regression ‚Üí minimize MSE / variance.\n",
    "* **Hyperparameters:** control tree depth, splits, leaf size, and pruning to balance bias‚Äìvariance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352208e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
