{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d522e97",
   "metadata": {},
   "source": [
    "\n",
    "**Understanding the Cost Function in Support Vector Classifier (SVC)**\n",
    "\n",
    "Hello everyone!  \n",
    "So far, we’ve discussed the **cost function** in the context of a Support Vector Machine (SVM), specifically for the **Support Vector Classifier (SVC)**.  \n",
    "\n",
    "### Objective  \n",
    "\n",
    "\n",
    "### **Hard Margin**\n",
    "Our primary goal in SVC is to **maximize** $\\frac{2}{\\|w\\|}$ (where $\\|w\\|$ is the magnitude of the weight vector $w$), by adjusting $w$ and $b$.  \n",
    "\n",
    "- This maximization increases the distance between the **marginal planes**, ensuring a better margin.  \n",
    "\n",
    "### Constraints\n",
    "\n",
    "1. **True Labels Constraint:**\n",
    "   - For $w^T x + b \\geq 1$: label the point as $+1$.  \n",
    "   - For $w^T x + b \\leq -1$: label the point as $-1$.  \n",
    "\n",
    "2. **Correct Points Constraint:**\n",
    "   - For all correctly classified points, the product $y_i(w^T x_i + b) \\geq 1$ must hold.  \n",
    "   - This ensures that correctly classified points lie on or beyond their respective margins.\n",
    "\n",
    "---\n",
    "\n",
    "### Refining the Cost Function\n",
    "\n",
    "To make the problem more tractable, we can **reformulate** the objective:  \n",
    "Instead of maximizing $\\frac{2}{\\|w\\|}$, we **minimize** $\\frac{\\|w\\|^2}{2}$, which is mathematically equivalent.  \n",
    "\n",
    "Thus, the **cost function** becomes:  \n",
    "$\n",
    "\\text{Minimize } \\frac{\\|w\\|^2}{2} \\text{ by adjusting } w \\text{ and } b.\n",
    "$\n",
    "\n",
    "#### Real-World Considerations\n",
    "\n",
    "In an ideal scenario, all points are **clearly separable**, and we can perfectly define the margins and the decision boundary. However, in real-world datasets, **overlapping points** and **misclassifications** are common.  \n",
    "\n",
    "For example:  \n",
    "\n",
    "- Some points may overlap or fall inside the margins.  \n",
    "- Therefore, we need to introduce additional **hyperparameters** to handle these scenarios effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Soft Margin SVM and the Enhanced Cost Function  \n",
    "\n",
    "To account for **overlapping points**, we modify the cost function by adding two components:  \n",
    "\n",
    "1. **Hyperparameter $C$:**\n",
    "   - $C$ determines how much we penalize misclassified points.  \n",
    "   - It reflects the **tolerance for misclassification**.  \n",
    "\n",
    "   For example:  \n",
    "   - If $C = 5$, we can allow up to **5 misclassified points**.  \n",
    "   - This helps balance the model’s ability to generalize and the strictness of classification.\n",
    "\n",
    "2. **Summation of $\\eta$:**\n",
    "   - $\\eta_i$ represents the **distance of misclassified points** from the margins.  \n",
    "   - The summation $\\sum_{i=1}^n \\eta_i$ calculates the **total deviation** of misclassified points.  \n",
    "\n",
    "Thus, the modified **cost function** becomes:  \n",
    "$\n",
    "\\text{Minimize } \\frac{\\|w\\|^2}{2} + C \\sum_{i=1}^n \\eta_i.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Components Explained  \n",
    "\n",
    "1. **Hyperparameter $C$:**  \n",
    "   - Controls how many points can be misclassified.  \n",
    "   - Higher $C$: Strict classification with fewer misclassifications.  \n",
    "   - Lower $C$: Allows more misclassifications, leading to better generalization.\n",
    "\n",
    "2. **$\\eta_i$:**  \n",
    "   - Measures the **distance of misclassified points** from the margins.  \n",
    "   - Helps ensure that misclassified points stay as close as possible to the margins.  \n",
    "\n",
    "---\n",
    "\n",
    "### Example Scenario\n",
    "\n",
    "Let’s say we set $C = 6$:  \n",
    "\n",
    "- This means we allow up to **6 misclassified points**.  \n",
    "- Even with these misclassifications, we aim to find the **best fit line** and the **marginal planes** that maximize separation.\n",
    "\n",
    "---\n",
    "\n",
    "### Hinge Loss\n",
    "\n",
    "The additional term $\\sum_{i=1}^n \\eta_i$ is referred to as the **hinge loss**, similar to how we use **log loss** in logistic regression.  \n",
    "\n",
    "- Hinge loss helps in managing misclassified points and ensuring that the model still generalizes well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddf4c3",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Hinge Loss Formulation (cost function per point)\n",
    "\n",
    "Another way to write the SVM cost is with **hinge loss**:\n",
    "\n",
    "For each data point:\n",
    "\n",
    "$$\n",
    "L_i = \\max(0, 1 - y_i(w \\cdot x_i + b))\n",
    "$$\n",
    "\n",
    "* If a point is correctly classified and outside the margin: loss = 0.\n",
    "* If it’s inside the margin or misclassified: loss grows linearly as it goes deeper into the wrong side.\n",
    "\n",
    "The full cost function:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n L_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Intuition with hinge loss\n",
    "\n",
    "* Think of hinge loss as a **springy fence** around the margin.\n",
    "* If points stay outside → no cost.\n",
    "* If they cross into the fence → you pay a penalty proportional to how far they intrude.\n",
    "\n",
    "---\n",
    "\n",
    "So: **SVM’s cost function = margin maximization (small weights) + hinge loss penalty for violations (scaled by C).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30201432",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
