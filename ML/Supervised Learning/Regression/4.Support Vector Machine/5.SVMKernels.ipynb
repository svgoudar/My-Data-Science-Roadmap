{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34263fa5",
      "metadata": {
        "id": "34263fa5"
      },
      "source": [
        "## 1. **Why Kernels?**\n",
        "\n",
        "* Linear SVM works only when data is *linearly separable*.\n",
        "* But what if classes are arranged in a circle, spiral, or any nonlinear shape?\n",
        "  â†’ A straight hyperplane wonâ€™t cut it.\n",
        "\n",
        "ðŸ’¡ Trick: Instead of working in the original feature space, **map data into a higher-dimensional space** where it becomes linearly separable.\n",
        "\n",
        "But explicitly computing this mapping can be expensive (even infinite-dimensional!).\n",
        "\n",
        "Thatâ€™s where kernels come in.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Kernel Trick**\n",
        "\n",
        "A **kernel function** computes the inner product in some higher-dimensional space *without ever computing the mapping explicitly*.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n",
        "$$\n",
        "\n",
        "* $\\phi(x)$ = mapping to higher-dim space\n",
        "* $K(x_i, x_j)$ = kernel function (efficient shortcut)\n",
        "\n",
        "This allows SVM to learn nonlinear boundaries **without heavy computation**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Common Kernels**\n",
        "\n",
        "Here are the most popular ones:\n",
        "\n",
        "### ðŸ”¹ (a) Linear Kernel\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = x_i^T x_j\n",
        "$$\n",
        "\n",
        "This formula defines the **linear kernel**. It simply computes the dot product between two input vectors $x_i$ and $x_j$. In the context of SVMs, using this kernel means no transformation is applied to the dataâ€”the classifier tries to find a straight-line (or hyperplane) boundary in the original feature space. This is equivalent to the standard linear SVM.\n",
        "\n",
        "* No transformation, just the dot product.\n",
        "* Same as linear SVM.\n",
        "  âœ… Use when features are already linearly separable.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcf5a03",
      "metadata": {
        "id": "bfcf5a03"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ (b) Polynomial Kernel\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = (x_i^T x_j + c)^d\n",
        "$$\n",
        "\n",
        "* $x_i, x_j$: Input feature vectors.\n",
        "* $x_i^T x_j$: Dot product between $x_i$ and $x_j$.\n",
        "* $c$: A constant that trades off the influence of higher-order versus lower-order terms (controls flexibility).\n",
        "* $d$: The degree of the polynomial (controls the complexity of the decision boundary).\n",
        "\n",
        "* Expands features into polynomials of degree $d$.\n",
        "* Example: in 2D, a quadratic kernel lets SVM draw circles, ellipses, etc.\n",
        "  âœ… Useful when interactions between features matter.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "---\n",
        "\n",
        "### ðŸ”¹ (c) Radial Basis Function (RBF) / Gaussian Kernel (Default)\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "\n",
        "* $x_i, x_j$: Input feature vectors.\n",
        "* $\\|x_i - x_j\\|^2$: Squared Euclidean distance between $x_i$ and $x_j$.\n",
        "* $\\sigma$: Controls the width of the Gaussian (how far the influence of a single training example reaches).\n",
        "* $\\exp$: Exponential function.\n",
        "\n",
        "* Measures similarity based on distance.\n",
        "* Points closer together have high kernel value.\n",
        "* Creates very flexible, nonlinear boundaries.\n",
        "  âœ… Most popular kernel when you donâ€™t know the data shape.\n",
        "\n",
        "![alt text](https://github.com/svgoudar/My-Data-Science-Roadmap/blob/main/ML/Supervised%20Learning/Regression/4.Support%20Vector%20Machine/image.png?raw=1)\n",
        "\n",
        "* K ~ 1 -- > High similarity\n",
        "* K ~ 0 -- > Low similarity\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ (d) Sigmoid Kernel\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c)\n",
        "$$\n",
        "\n",
        "* $x_i, x_j$: Input feature vectors.\n",
        "* $x_i^T x_j$: Dot product between $x_i$ and $x_j$.\n",
        "* $\\alpha$: Slope parameter (controls the \"steepness\" of the sigmoid).\n",
        "* $c$: Intercept parameter (shifts the function left/right).\n",
        "* $\\tanh$: Hyperbolic tangent function (squashes values between -1 and 1).\n",
        "\n",
        "* Similar to neural networksâ€™ activation.\n",
        "* Less common, not always positive semi-definite.\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "---\n",
        "\n",
        "## 4. **How Kernels Affect Decision Boundary**\n",
        "\n",
        "* Linear kernel â†’ straight line.\n",
        "* Polynomial kernel â†’ curved surfaces (parabolas, circles, etc.).\n",
        "* RBF kernel â†’ very flexible, can carve complex shapes.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. **Choosing the Right Kernel**\n",
        "\n",
        "* **Linear kernel**: many features, but linearly separable.\n",
        "* **Polynomial kernel**: medium complexity, feature interactions.\n",
        "* **RBF kernel**: general-purpose, works well in most cases.\n",
        "* **Sigmoid**: rarely used today (neural nets do it better).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. **SVM with Kernel Decision Function**\n",
        "\n",
        "Final classifier is:\n",
        "\n",
        "$$\n",
        "f(x) = \\text{sign}\\Big( \\sum_i \\alpha_i y_i K(x_i, x) + b \\Big)\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $x$: The input vector to classify.\n",
        "- $x_i$: The $i$-th support vector from the training set.\n",
        "- $\\alpha_i$: Learned weight for the $i$-th support vector (from training).\n",
        "- $y_i$: Label (+1 or -1) of the $i$-th support vector.\n",
        "- $K(x_i, x)$: Kernel function measuring similarity between $x_i$ and $x$.\n",
        "- $b$: Bias term (intercept).\n",
        "- $\\text{sign}(\\cdot)$: Returns +1 or -1, indicating the predicted class.\n",
        "\n",
        "So only support vectors + kernel function are needed at prediction time.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸ‘‰ In short: **Kernels let SVM \"bend\" the decision boundary by secretly lifting data into higher dimensions, but only doing math in the original space.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74918979",
      "metadata": {
        "id": "74918979"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}