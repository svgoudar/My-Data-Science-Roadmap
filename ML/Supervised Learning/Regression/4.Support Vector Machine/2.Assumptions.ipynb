{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c2e9f6",
   "metadata": {},
   "source": [
    "### 1. **Data should be (at least approximately) separable**\n",
    "\n",
    "* Hard-margin SVM assumes perfect linear separability.\n",
    "* Soft-margin SVM relaxes this: allows some misclassifications, but still expects that classes are roughly separable with a margin.\n",
    "* If the classes are hopelessly mixed, even kernels will struggle.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Margin maximization assumption**\n",
    "\n",
    "* SVM assumes that the “best” boundary is the one that maximizes the margin.\n",
    "* This works well in many scenarios but may not match the true data-generating process (e.g., overlapping Gaussians where probability-based models like logistic regression may do better).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Feature scaling matters**\n",
    "\n",
    "* SVM assumes features are on comparable scales because distances (dot products, kernels) drive the margin.\n",
    "* Without scaling, one feature can dominate the geometry.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Independent and identically distributed (i.i.d.) data**\n",
    "\n",
    "* As with most supervised learning methods, SVM assumes the training samples are independent draws from the underlying distribution.\n",
    "* Violations (e.g., strong time dependence) can break the validity of the margin.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Choice of kernel reflects data structure**\n",
    "\n",
    "* Linear kernel assumes linear separability in the original space.\n",
    "* Polynomial, RBF (Gaussian), sigmoid kernels assume the data can be separated in some higher-dimensional feature space.\n",
    "* Picking the wrong kernel is like wearing the wrong glasses—everything looks blurry.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Balanced class distributions (ideally)**\n",
    "\n",
    "* SVM doesn’t naturally handle severe class imbalance.\n",
    "* If one class massively outweighs the other, the margin may skew. Class weights or resampling are usually needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Low noise assumption (especially for hard margin)**\n",
    "\n",
    "* SVM assumes labels are reliable.\n",
    "* Noisy or mislabeled points can become misleading support vectors and twist the margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253e211",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
