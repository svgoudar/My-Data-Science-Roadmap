{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564b01b7",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Assumptions in Gradient Boosting\n",
    "\n",
    "### 1. **Additive Model Assumption**\n",
    "\n",
    "* The final predictor can be written as:\n",
    "\n",
    "  $$\n",
    "  F_M(x) = \\sum_{m=1}^M \\nu \\cdot h_m(x)\n",
    "  $$\n",
    "\n",
    "  where $h_m(x)$ are weak learners (often decision trees).\n",
    "* We assume that **incrementally adding weak models improves performance** â†’ i.e., the target function is approximable by an additive expansion of weak learners.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Weak Learner Assumption**\n",
    "\n",
    "* Base learners (usually shallow trees) should be **slightly better than random guessing**.\n",
    "* If weak learners are too strong (deep trees), GBM may overfit fast. If too weak (random predictions), boosting wonâ€™t converge.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Gradient Approximation Assumption**\n",
    "\n",
    "* Boosting fits new learners to the **negative gradient of the loss function**.\n",
    "* Assumes that this approximation step is valid â†’ i.e., weak learners can **reasonably approximate the gradient direction** in function space.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Independent & Identically Distributed (i.i.d.) Data**\n",
    "\n",
    "* Standard ML assumption: training samples are **independent** and come from the same distribution as test data.\n",
    "* If data distribution shifts (non-stationary data), boosting may fail.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Loss Function is Differentiable**\n",
    "\n",
    "* Gradient Boosting assumes the **loss function is differentiable** wrt predictions, so we can compute gradients.\n",
    "\n",
    "  * For regression: MSE, MAE, Huber loss.\n",
    "  * For classification: Logistic loss, exponential loss.\n",
    "* (Though there are tricks for non-differentiable losses like ranking loss in ranking problems.)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Biasâ€“Variance Tradeoff Assumption**\n",
    "\n",
    "* Boosting assumes sequential corrections (reducing bias) improve generalization without exploding variance.\n",
    "* Hence learning rate, tree depth, and number of iterations must balance bias vs variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Noisy Data Sensitivity Assumption**\n",
    "\n",
    "* Gradient Boosting assumes **errors are learnable**.\n",
    "* If thereâ€™s a lot of **label noise**, boosting will overfit (because it keeps focusing on mistakes, even when theyâ€™re due to noise).\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… Summary\n",
    "\n",
    "Gradient Boosting assumes:\n",
    "\n",
    "1. The target function can be approximated by an additive model.\n",
    "2. Weak learners are slightly better than random.\n",
    "3. Gradients can be reasonably approximated by trees.\n",
    "4. Data is i.i.d.\n",
    "5. Loss is differentiable.\n",
    "6. Biasâ€“variance tradeoff can be tuned.\n",
    "7. Errors are meaningful (not pure noise).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d41fc9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
