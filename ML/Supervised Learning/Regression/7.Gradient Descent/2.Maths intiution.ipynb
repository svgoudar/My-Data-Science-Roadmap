{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e942ecd",
   "metadata": {},
   "source": [
    "# ðŸ”¹ 1. Setup\n",
    "\n",
    "We want to learn a function $F(x)$ that predicts $y$ given input $x$.\n",
    "\n",
    "* Data: $\\{(x_i, y_i)\\}_{i=1}^N$\n",
    "* Loss function: $L(y, F(x))$\n",
    "\n",
    "Our goal is to minimize the **empirical risk**:\n",
    "\n",
    "$$\n",
    "\\min_{F} \\; \\sum_{i=1}^N L(y_i, F(x_i))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ 2. Idea of Gradient Boosting\n",
    "\n",
    "Gradient Boosting builds $F(x)$ **additively** as:\n",
    "\n",
    "$$\n",
    "F_M(x) = F_0(x) + \\sum_{m=1}^M \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "* $F_0(x)$ = initial model (e.g., constant mean for regression, log-odds for classification).\n",
    "* At each step $m$, we add a new weak learner $h_m(x)$ scaled by weight $\\gamma_m$.\n",
    "* Each $h_m(x)$ is chosen to reduce the loss as much as possible.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ 3. Gradient Descent Analogy\n",
    "\n",
    "In optimization, gradient descent updates parameters in the **negative gradient direction**:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta L\n",
    "$$\n",
    "\n",
    "In **Gradient Boosting**, we donâ€™t have parameters like $\\theta$. Instead, our \"parameter\" is the **function** $F(x)$. So we do **functional gradient descent**:\n",
    "\n",
    "$$\n",
    "F_{m}(x) = F_{m-1}(x) - \\eta \\cdot g_m(x)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "g_m(x_i) = \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m-1}}\n",
    "$$\n",
    "\n",
    "That is, the gradient is taken with respect to the **predictions**, not parameters.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ 4. Step-by-Step Boosting Algorithm\n",
    "\n",
    "At iteration $m$:\n",
    "\n",
    "1. **Compute pseudo-residuals** (negative gradient):\n",
    "\n",
    "   $$\n",
    "   r_{im} = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m-1}}\n",
    "   $$\n",
    "\n",
    "2. **Fit a weak learner** $h_m(x)$ to approximate $r_{im}$.\n",
    "\n",
    "   * Usually $h_m(x)$ is a small decision tree.\n",
    "\n",
    "3. **Compute optimal step size** (line search):\n",
    "\n",
    "   $$\n",
    "   \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^N L\\big(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\big)\n",
    "   $$\n",
    "\n",
    "4. **Update the model**:\n",
    "\n",
    "   $$\n",
    "   F_m(x) = F_{m-1}(x) + \\eta \\gamma_m h_m(x)\n",
    "   $$\n",
    "\n",
    "where $\\eta$ = learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ 5. Special Cases\n",
    "\n",
    "* **Squared Error Loss** (Regression):\n",
    "\n",
    "  $$\n",
    "  L(y, F(x)) = \\frac{1}{2}(y - F(x))^2\n",
    "  $$\n",
    "\n",
    "  Gradient:\n",
    "\n",
    "  $$\n",
    "  r_{im} = y_i - F_{m-1}(x_i) \\quad (\\text{residuals!})\n",
    "  $$\n",
    "\n",
    "  ðŸ‘‰ Gradient Boosting reduces to \"fit trees to residuals\".\n",
    "\n",
    "* **Logistic Loss** (Classification):\n",
    "\n",
    "  $$\n",
    "  L(y, F(x)) = \\log(1 + e^{-yF(x)})\n",
    "  $$\n",
    "\n",
    "  Gradient:\n",
    "\n",
    "  $$\n",
    "  r_{im} = \\frac{y_i}{1 + e^{y_i F_{m-1}(x_i)}}\n",
    "  $$\n",
    "\n",
    "  ðŸ‘‰ Trees are fit to these residual-like terms.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¹ 6. Intuition in Words\n",
    "\n",
    "* Gradient Boosting = **Gradient Descent in function space**.\n",
    "* Each weak learner = \"step in negative gradient direction\".\n",
    "* Learning rate $\\eta$ = step size.\n",
    "* Residuals = \"errors the model still needs to fix\".\n",
    "\n",
    "---\n",
    "\n",
    "âœ… So, Gradient Boosting = iteratively fitting weak learners to the **gradient of the loss function** with respect to predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98265e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
