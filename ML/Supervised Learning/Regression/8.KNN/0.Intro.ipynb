{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7bd1c2",
   "metadata": {},
   "source": [
    "### **1. What is KNN?**\n",
    "\n",
    "K-Nearest Neighbors is a **simple, instance-based, supervised machine learning algorithm** used for **classification** and **regression**.\n",
    "\n",
    "* **Instance-based** means it doesn’t explicitly learn a model; it memorizes the training data.\n",
    "* **Supervised** means it requires labeled data for training.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How KNN Works**\n",
    "\n",
    "The core idea:\n",
    "\n",
    "> To predict the label of a new point, look at the **K closest points** (neighbors) in the training set and take a **majority vote** (for classification) or **average** (for regression).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Choose a value of **K** (number of neighbors to consider).\n",
    "2. Compute the **distance** between the new point and all points in the training set.\n",
    "\n",
    "   * Common distance metrics:\n",
    "\n",
    "     * **Euclidean distance**: $\\sqrt{\\sum (x_i - y_i)^2}$\n",
    "     * **Manhattan distance**: $\\sum |x_i - y_i|$\n",
    "3. Identify the **K nearest neighbors**.\n",
    "4. **Classification:** Take the most common class among neighbors.\n",
    "   **Regression:** Take the average of neighbors' values.\n",
    "5. Assign this as the predicted label/value.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Example (Classification)**\n",
    "\n",
    "Suppose you want to classify a fruit as **Apple** or **Orange** based on features like weight and color:\n",
    "\n",
    "| Weight | Color  | Label  |\n",
    "| ------ | ------ | ------ |\n",
    "| 150    | Red    | Apple  |\n",
    "| 170    | Red    | Apple  |\n",
    "| 140    | Orange | Orange |\n",
    "| 160    | Orange | Orange |\n",
    "\n",
    "* New fruit: weight = 155, color = Red\n",
    "* Compute distances to all points.\n",
    "* Choose **K=3** nearest neighbors: maybe 2 Apples, 1 Orange.\n",
    "* Predicted label = **Apple** (majority vote).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Pros of KNN**\n",
    "\n",
    "* Simple to understand and implement.\n",
    "* No training phase (lazy learner).\n",
    "* Naturally handles multi-class problems.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Cons of KNN**\n",
    "\n",
    "* Computationally expensive for large datasets (distance computed for all points).\n",
    "* Sensitive to **feature scaling** (need normalization).\n",
    "* Choosing the right **K** is critical:\n",
    "\n",
    "  * Small K → sensitive to noise (overfitting).\n",
    "  * Large K → may smooth out patterns (underfitting).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Tips**\n",
    "\n",
    "* Always **normalize/standardize features** before using KNN.\n",
    "* Use **cross-validation** to choose the best K.\n",
    "* Consider **distance weighting**: closer neighbors have more influence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d195e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
