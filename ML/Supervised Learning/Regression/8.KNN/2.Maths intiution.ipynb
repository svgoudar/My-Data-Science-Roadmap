{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e942ecd",
   "metadata": {},
   "source": [
    "## **1. Distance Metrics**\n",
    "\n",
    "KNN relies on the concept of **distance** to determine which points are ‚Äúneighbors.‚Äù The most common metric is **Euclidean distance**:\n",
    "\n",
    "$$\n",
    "d(x_i, x_j) = \\sqrt{\\sum_{k=1}^{n} (x_{ik} - x_{jk})^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_i, x_j$ = two points in $n$-dimensional feature space\n",
    "* $k$ = feature index\n",
    "\n",
    "Other metrics include:\n",
    "\n",
    "* **Manhattan distance**: $d(x_i, x_j) = \\sum_{k=1}^{n} |x_{ik} - x_{jk}|$\n",
    "* **Minkowski distance** (generalized): $d(x_i, x_j) = (\\sum_{k=1}^{n} |x_{ik} - x_{jk}|^p)^{1/p}$\n",
    "\n",
    "The distance determines **‚Äúcloseness‚Äù** mathematically.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Classification Prediction**\n",
    "\n",
    "Suppose we have **K neighbors** of a test point $x$: $\\{x_1, x_2, ..., x_K\\}$ with labels $\\{y_1, y_2, ..., y_K\\}$.\n",
    "\n",
    "The predicted class $\\hat{y}$ is the **majority vote**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{mode}(y_1, y_2, ..., y_K)\n",
    "$$\n",
    "\n",
    "Optional: weighted voting gives more weight to closer neighbors:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{argmax}_c<span title=\"returns the value of c that maximizes the sum\">*</span> \\sum_{i \\in K} w_i \\cdot \\mathbf{1}(y_i = c)\n",
    "$$\n",
    "\n",
    "[^bignote]: \"argmax\" returns the value of \\(c\\) that maximizes the expression.\n",
    "\n",
    "Where:\n",
    "\n",
    "* $w_i = \\frac{1}{d(x, x_i)}$ (inverse distance weighting)\n",
    "* $\\mathbf{1}(\\cdot)$ = indicator function\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Regression Prediction**\n",
    "\n",
    "For regression, KNN predicts the **average of the neighbors**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{K} \\sum_{i=1}^{K} y_i\n",
    "$$\n",
    "\n",
    "Weighted version:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{\\sum_{i=1}^{K} w_i y_i}{\\sum_{i=1}^{K} w_i}\n",
    "$$\n",
    "\n",
    "* Closer neighbors have larger weights ($w_i$), influencing the prediction more.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Key Intuition**\n",
    "\n",
    "1. **Local Approximation**:\n",
    "\n",
    "   * KNN approximates the function $f(x)$ locally using nearby points.\n",
    "   * It assumes the function is **smooth in small neighborhoods**.\n",
    "\n",
    "2. **Non-parametric**:\n",
    "\n",
    "   * No assumption about global data distribution.\n",
    "   * The function $f(x)$ is ‚Äúlearned‚Äù directly from training points.\n",
    "\n",
    "3. **Bias-Variance Tradeoff**:\n",
    "\n",
    "   * **Small K** ‚Üí low bias, high variance (sensitive to noise).\n",
    "   * **Large K** ‚Üí high bias, low variance (smooths out local details).\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Summary in one line**:\n",
    "\n",
    "> KNN uses **distance in feature space** to find neighbors and predicts based on **majority vote or weighted average**, approximating the function **locally**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98265e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
