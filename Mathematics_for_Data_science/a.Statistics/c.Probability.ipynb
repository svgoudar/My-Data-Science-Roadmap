{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0569b7",
   "metadata": {},
   "source": [
    "\n",
    "## Probability Theory (Core Understanding) - Deeper Dive\n",
    "\n",
    "Probability theory is the mathematical framework for quantifying uncertainty. It allows us to make informed decisions and predictions in situations where outcomes are not predetermined.\n",
    "\n",
    "### Basics\n",
    "\n",
    "#### Sample Space ($\\Omega$ or S) - *A Universe of Possibilities*\n",
    "\n",
    "The sample space is the foundational concept. It's the complete, exhaustive, and mutually exclusive set of all possible outcomes of a random experiment. Think of it as the \"universe\" for that particular experiment.\n",
    "\n",
    "* **Characteristics of a Sample Space:**\n",
    "    * **Exhaustive:** It must include every single possible outcome. Nothing can be left out.\n",
    "    * **Mutually Exclusive:** No two outcomes in the sample space can occur at the same time. If one outcome happens, all others in the sample space are automatically excluded.\n",
    "    * **Properly Defined:** The outcomes should be distinct and unambiguous.\n",
    "\n",
    "* **Types of Sample Spaces:**\n",
    "    * **Discrete Sample Space:** The outcomes are countable, often finite, or countably infinite (like the set of positive integers).\n",
    "        * *Example:* Number of heads in 3 coin flips: $\\Omega = \\{0, 1, 2, 3\\}$ (Finite)\n",
    "        * *Example:* Number of flips until the first head: $\\Omega = \\{1, 2, 3, \\dots\\}$ (Countably infinite)\n",
    "    * **Continuous Sample Space:** The outcomes can take any value within a given range (an interval). These are typically uncountable.\n",
    "        * *Example:* The exact temperature of a room: $\\Omega = \\{x \\mid x \\in \\mathbb{R}, x > 0\\}$ (or a specific range like $[15^\\circ C, 30^\\circ C]$)\n",
    "        * *Example:* The height of a randomly selected person.\n",
    "\n",
    "* **Importance:** A clearly defined sample space is crucial because all probability calculations are based on it. If you miss an outcome, your probabilities will be incorrect.\n",
    "\n",
    "#### Events (E) - *Specific Subsets of Outcomes*\n",
    "\n",
    "An event is a subset of the sample space. It's a collection of one or more outcomes that we are interested in. Events are often denoted by capital letters like A, B, C.\n",
    "\n",
    "* **Types of Events:**\n",
    "    * **Simple Event (Elementary Event):** An event consisting of exactly one outcome from the sample space.\n",
    "        * *Example:* Rolling a die and getting a 3. $E = \\{3\\}$\n",
    "    * **Compound Event:** An event consisting of two or more outcomes from the sample space.\n",
    "        * *Example:* Rolling a die and getting an even number. $E = \\{2, 4, 6\\}$\n",
    "    * **Certain Event:** An event that is guaranteed to happen. It is equal to the sample space itself. $P(\\text{Certain Event}) = 1$.\n",
    "        * *Example:* Rolling a die and getting a number less than 7. $E = \\{1, 2, 3, 4, 5, 6\\} = \\Omega$.\n",
    "    * **Impossible Event:** An event that cannot happen. It is represented by an empty set ($\\emptyset$). $P(\\text{Impossible Event}) = 0$.\n",
    "        * *Example:* Rolling a die and getting a 7. $E = \\{\\}$.\n",
    "\n",
    "* **Operations on Events:**\n",
    "    * **Union ($A \\cup B$):** Occurs if event A *or* event B (or both) occur.\n",
    "    * **Intersection ($A \\cap B$):** Occurs if event A *and* event B both occur.\n",
    "    * **Complement ($A^c$ or $A'$):** Occurs if event A *does not* occur. (As explained in the previous response).\n",
    "\n",
    "#### Types of Probability - *Different Ways to Quantify Likelihood*\n",
    "\n",
    "##### Classical Probability (A Priori Probability) - *Based on Logic and Symmetry*\n",
    "\n",
    "This is the oldest and most straightforward approach, assuming a perfectly balanced or fair system where all outcomes are equally likely. It's \"a priori\" because you can determine the probability *before* any experiment is performed, simply by reasoning.\n",
    "\n",
    "* **Core Assumption:** Each outcome in the sample space has an equal chance of occurring. This is a strong assumption and limits its applicability to ideal scenarios.\n",
    "* **Calculation:**\n",
    "    $P(E) = \\frac{\\text{Number of favorable outcomes for event E}}{\\text{Total number of distinct, equally likely outcomes in the sample space}}$\n",
    "* **Detailed Example:** A bag contains 5 red marbles, 3 blue marbles, and 2 green marbles. What is the probability of drawing a blue marble?\n",
    "    * Total outcomes (marbles): $5 + 3 + 2 = 10$.\n",
    "    * Favorable outcomes (blue marbles): 3.\n",
    "    * $P(\\text{Blue Marble}) = \\frac{3}{10} = 0.3$.\n",
    "* **Limitations:** Not applicable when outcomes are not equally likely (e.g., a loaded die) or when the sample space is infinitely large (e.g., continuous variables).\n",
    "\n",
    "##### Empirical Probability (A Posteriori Probability or Relative Frequency) - *Based on Observation and Experimentation*\n",
    "\n",
    "This type of probability is derived from actual data, observations, or experiments. It's \"a posteriori\" because it's determined *after* an experiment has been conducted.\n",
    "\n",
    "* **Core Idea:** The probability of an event is estimated by its observed frequency in a long series of trials.\n",
    "* **Calculation:**\n",
    "    $P(E) = \\frac{\\text{Number of times event E occurred in trials}}{\\text{Total number of trials}}$\n",
    "* **Detailed Example:** A quality control inspector checks 500 light bulbs and finds 15 of them are defective. What is the empirical probability that a randomly chosen light bulb from this production batch is defective?\n",
    "    * Number of defective bulbs (favorable outcomes): 15\n",
    "    * Total bulbs inspected (total trials): 500\n",
    "    * $P(\\text{Defective}) = \\frac{15}{500} = \\frac{3}{100} = 0.03$.\n",
    "* **Law of Large Numbers:** A crucial concept here. As the number of trials in an experiment increases, the empirical probability of an event tends to get closer and closer to its true (theoretical) probability. This is why polling larger samples often leads to more accurate predictions.\n",
    "* **Applications:** Widely used in science, engineering, finance, insurance (actuarial science), and public health.\n",
    "\n",
    "##### Subjective Probability - *Based on Personal Belief and Expert Judgment*\n",
    "\n",
    "This is the least formal but often necessary type of probability. It reflects an individual's personal assessment of the likelihood of an event, based on available information, intuition, experience, and sometimes even biases.\n",
    "\n",
    "* **Core Idea:** It quantifies a degree of belief.\n",
    "* **Characteristics:**\n",
    "    * **Personal:** Different individuals may assign different subjective probabilities to the same event.\n",
    "    * **Dynamic:** Can change as new information becomes available.\n",
    "    * **Not Falsifiable by Single Event:** You can't prove a subjective probability wrong with one outcome (e.g., if a meteorologist says 70% chance of rain and it doesn't rain, their prediction isn't necessarily \"wrong\" if their reasoning was sound).\n",
    "* **Detailed Example:**\n",
    "    * A venture capitalist estimates a 40% chance that a particular startup will become profitable within five years, based on the business plan, team, market analysis, and their past experience.\n",
    "    * A jury member assigns a 90% probability that the defendant is guilty based on the presented evidence.\n",
    "    * A poker player estimates the probability of their opponent having a certain hand.\n",
    "* **Applications:** Decision-making under uncertainty, particularly in business strategy, legal judgments, sports betting, and personal finance where objective data might be scarce or inconclusive.\n",
    "\n",
    "### Rules of Probability - *Combining and Relating Events*\n",
    "\n",
    "These rules are the bedrock for calculating probabilities of complex events.\n",
    "\n",
    "#### Addition Rule (for \"OR\" events - Union of Events)\n",
    "\n",
    "This rule helps calculate the probability that at least one of two (or more) events occurs.\n",
    "\n",
    "* **Mutually Exclusive Events (Disjoint Events):** Events that cannot occur at the same time. Their intersection is empty.\n",
    "    * *Formal Definition:* $A \\cap B = \\emptyset$\n",
    "    * *Formula:* $P(A \\cup B) = P(A) + P(B)$\n",
    "    * *Deeper Example:* In a deck of cards, drawing a King (Event A) and drawing a Queen (Event B) are mutually exclusive. You cannot draw a card that is both a King and a Queen.\n",
    "        $P(\\text{King or Queen}) = P(\\text{King}) + P(\\text{Queen}) = \\frac{4}{52} + \\frac{4}{52} = \\frac{8}{52} = \\frac{2}{13}$\n",
    "\n",
    "* **Non-Mutually Exclusive Events (Overlapping Events):** Events that can occur at the same time. Their intersection is not empty.\n",
    "    * *Formal Definition:* $A \\cap B \\neq \\emptyset$\n",
    "    * *Formula:* $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n",
    "        * The subtraction of $P(A \\cap B)$ is crucial to avoid double-counting the outcomes that are common to both A and B.\n",
    "    * *Deeper Example:* What is the probability of drawing a face card (Jack, Queen, King) OR a heart from a standard deck?\n",
    "        * Event A: Drawing a Face Card. There are 12 face cards (3 per suit). $P(A) = \\frac{12}{52}$\n",
    "        * Event B: Drawing a Heart. There are 13 hearts. $P(B) = \\frac{13}{52}$\n",
    "        * Event $A \\cap B$: Drawing a face card *and* a heart. These are the King of Hearts, Queen of Hearts, Jack of Hearts. There are 3 such cards. $P(A \\cap B) = \\frac{3}{52}$\n",
    "        * $P(\\text{Face Card or Heart}) = P(A) + P(B) - P(A \\cap B) = \\frac{12}{52} + \\frac{13}{52} - \\frac{3}{52} = \\frac{25 - 3}{52} = \\frac{22}{52} = \\frac{11}{26}$\n",
    "\n",
    "#### Multiplication Rule (for \"AND\" events - Intersection of Events)\n",
    "\n",
    "This rule calculates the probability that two or more events all occur.\n",
    "\n",
    "* **Independent Events:** The occurrence of one event does not influence the probability of the other event occurring.\n",
    "    * *Formal Definition:* $P(B|A) = P(B)$ (The probability of B given A is just the probability of B).\n",
    "    * *Formula:* $P(A \\cap B) = P(A) \\times P(B)$\n",
    "    * *Deeper Example:* Rolling a die and flipping a coin. What is the probability of rolling a 6 AND getting a Head?\n",
    "        * $P(\\text{6}) = \\frac{1}{6}$\n",
    "        * $P(\\text{Head}) = \\frac{1}{2}$\n",
    "        * These events are independent.\n",
    "        * $P(\\text{6 and Head}) = P(\\text{6}) \\times P(\\text{Head}) = \\frac{1}{6} \\times \\frac{1}{2} = \\frac{1}{12}$\n",
    "\n",
    "* **Dependent Events:** The occurrence of one event *does* affect the probability of the other event occurring.\n",
    "    * *Formula:* $P(A \\cap B) = P(A) \\times P(B|A)$\n",
    "        * This can be extended for more than two events: $P(A \\cap B \\cap C) = P(A) \\times P(B|A) \\times P(C|A \\cap B)$\n",
    "    * *Deeper Example:* Drawing two cards *without replacement* from a deck. What is the probability of drawing two Queens?\n",
    "        * Event A: Drawing a Queen on the first draw. $P(A) = \\frac{4}{52}$\n",
    "        * Event B: Drawing a Queen on the second draw *given* the first was a Queen and not replaced.\n",
    "        * After the first Queen is drawn, there are 3 Queens left and 51 total cards.\n",
    "        * $P(B|A) = \\frac{3}{51}$\n",
    "        * $P(\\text{Queen and Queen}) = P(A) \\times P(B|A) = \\frac{4}{52} \\times \\frac{3}{51} = \\frac{12}{2652} = \\frac{1}{221}$\n",
    "\n",
    "#### Conditional Probability - *Probability Under New Information*\n",
    "\n",
    "This is a critical concept that quantifies how the probability of an event changes when we know that another event has already occurred. It narrows down the sample space.\n",
    "\n",
    "* **Formula:** $P(B|A) = \\frac{P(A \\cap B)}{P(A)}$\n",
    "    * The \"condition\" is that event A has already happened. We are now considering only the outcomes within A.\n",
    "    * $P(A \\cap B)$ represents the outcomes where *both* A and B occur.\n",
    "    * $P(A)$ normalizes this by the size of the new (reduced) sample space (event A).\n",
    "* **Intuition:** Imagine you're looking at a subset of your original sample space. $P(B|A)$ tells you the proportion of that subset where B also occurs.\n",
    "* **Deeper Example:** A group of 100 students: 60 study Math, 40 study Science, and 20 study both.\n",
    "    * $P(\\text{Math}) = 60/100 = 0.6$\n",
    "    * $P(\\text{Science}) = 40/100 = 0.4$\n",
    "    * $P(\\text{Math and Science}) = 20/100 = 0.2$\n",
    "    * What is the probability that a student studies Science GIVEN that they study Math? ($P(\\text{Science}|\\text{Math})$)\n",
    "        * We are now only considering the 60 students who study Math. Of those 60, 20 also study Science.\n",
    "        * Using the formula: $P(\\text{Science}|\\text{Math}) = \\frac{P(\\text{Math and Science})}{P(\\text{Math})} = \\frac{0.2}{0.6} = \\frac{1}{3} \\approx 0.333$\n",
    "        * This makes sense: among the 60 math students, 20 also do science. $20/60 = 1/3$.\n",
    "\n",
    "#### Bayes’ Theorem - *Updating Beliefs with Evidence*\n",
    "\n",
    "Bayes' Theorem is a cornerstone of inferential statistics and machine learning. It provides a way to update the probability of a hypothesis (event A) when new evidence (event B) becomes available. It's about reversing conditional probabilities.\n",
    "\n",
    "* **Formula:** $P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n",
    "    * $P(A|B)$: **Posterior Probability** - The probability of our hypothesis (A) being true, given the new evidence (B). This is what we want to find.\n",
    "    * $P(B|A)$: **Likelihood** - The probability of observing the evidence (B) if our hypothesis (A) were true. This comes from our model or knowledge.\n",
    "    * $P(A)$: **Prior Probability** - Our initial belief about the probability of the hypothesis (A) being true, *before* observing the new evidence.\n",
    "    * $P(B)$: **Marginal Probability of Evidence** - The total probability of observing the evidence (B), regardless of whether A is true or not. This acts as a normalizing constant. It can be expanded as:\n",
    "        $P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$ (where $A^c$ is the complement of A, i.e., A is false).\n",
    "\n",
    "* **Power of Bayes' Theorem:** It's how we learn from data. We start with a prior belief, observe some data, and then use Bayes' Theorem to get a more informed posterior belief.\n",
    "\n",
    "* **Deeper Example: Drug Testing**\n",
    "    * A drug test has a 99% true positive rate ($P(\\text{Positive}|\\text{User}) = 0.99$) and a 1% false positive rate ($P(\\text{Positive}|\\text{Non-User}) = 0.01$).\n",
    "    * 1% of the population uses the drug ($P(\\text{User}) = 0.01$).\n",
    "    * If someone tests positive, what is the probability they actually use the drug? ($P(\\text{User}|\\text{Positive})$)\n",
    "\n",
    "    * Let $A = \\text{User}$ (has the drug)\n",
    "    * Let $B = \\text{Positive}$ (tests positive)\n",
    "\n",
    "    * We know:\n",
    "        * $P(A) = 0.01$ (Prior probability of being a user)\n",
    "        * $P(A^c) = 1 - P(A) = 0.99$ (Prior probability of being a non-user)\n",
    "        * $P(B|A) = 0.99$ (Likelihood: probability of positive test given user)\n",
    "        * $P(B|A^c) = 0.01$ (Likelihood: probability of positive test given non-user - false positive)\n",
    "\n",
    "    * First, calculate $P(B)$ (Total probability of testing positive):\n",
    "        $P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$\n",
    "        $P(B) = (0.99)(0.01) + (0.01)(0.99) = 0.0099 + 0.0099 = 0.0198$\n",
    "\n",
    "    * Now, apply Bayes' Theorem:\n",
    "        $P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} = \\frac{0.99 \\times 0.01}{0.0198} = \\frac{0.0099}{0.0198} = 0.5$\n",
    "\n",
    "    * **Interpretation:** Even with a positive test, there's only a 50% chance the person actually uses the drug! This counter-intuitive result highlights the importance of the prior probability ($P(A)$) and the false positive rate. Since drug use is rare (1%), a significant portion of positive tests come from false positives among the large non-user population.\n",
    "\n",
    "### Counting Principles - *The Art of Enumeration*\n",
    "\n",
    "Before calculating probabilities, we often need to know \"how many\" possible outcomes or favorable outcomes exist. This is where counting principles come in.\n",
    "\n",
    "#### Permutations - *Order Matters!*\n",
    "\n",
    "A permutation is an arrangement of objects in a specific sequence. The key differentiator is that changing the order creates a *new* permutation.\n",
    "\n",
    "* **When to use:** When you are arranging items, assigning positions, or when the sequence of selection is important.\n",
    "* **Formula for Permutations of n objects taken r at a time:**\n",
    "    $P(n, r) = \\frac{n!}{(n-r)!}$\n",
    "    Where:\n",
    "    * $n$: total number of distinct objects available.\n",
    "    * $r$: number of objects being selected and arranged.\n",
    "    * $n!$ (n factorial) is $n \\times (n-1) \\times (n-2) \\times \\dots \\times 2 \\times 1$. $0! = 1$.\n",
    "\n",
    "* **Deeper Example:** How many ways can 4 different books be arranged on a shelf?\n",
    "    * n = 4 (total books)\n",
    "    * r = 4 (all books are being arranged)\n",
    "    * $P(4, 4) = \\frac{4!}{(4-4)!} = \\frac{4!}{0!} = \\frac{4 \\times 3 \\times 2 \\times 1}{1} = 24$ ways.\n",
    "\n",
    "* **Permutations with Repetition:** If you have repeated items, the formula changes. For example, for the word \"MISSISSIPPI\", you'd account for repeated 'S', 'I', 'P'.\n",
    "    * Formula for n objects with $n_1$ identical objects of type 1, $n_2$ identical objects of type 2, etc.: $\\frac{n!}{n_1!n_2!\\dots n_k!}$\n",
    "\n",
    "#### Combinations - *Order Doesn't Matter!*\n",
    "\n",
    "A combination is a selection of objects where the order of selection does not matter. It's about forming groups or subsets.\n",
    "\n",
    "* **When to use:** When you are choosing a committee, picking lottery numbers, or selecting a group where the sequence of selection doesn't create a new outcome.\n",
    "* **Formula for Combinations of n objects taken r at a time:**\n",
    "    $C(n, r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}$\n",
    "    * This formula effectively divides the number of permutations by $r!$ to remove the orderings within each group of $r$ objects.\n",
    "\n",
    "* **Deeper Example:** You have 10 friends, and you want to invite 3 of them to a dinner party. How many different groups of 3 friends can you invite?\n",
    "    * n = 10 (total friends)\n",
    "    * r = 3 (friends to invite)\n",
    "    * Order doesn't matter (inviting A, B, C is the same as inviting B, A, C).\n",
    "    * $C(10, 3) = \\frac{10!}{3!(10-3)!} = \\frac{10!}{3!7!} = \\frac{10 \\times 9 \\times 8 \\times 7!}{ (3 \\times 2 \\times 1) \\times 7!} = \\frac{10 \\times 9 \\times 8}{3 \\times 2 \\times 1} = \\frac{720}{6} = 120$ groups.\n",
    "\n",
    "* **Key Distinction between Permutations and Combinations:**\n",
    "    * **Permutation:** Choosing a president, vice-president, and secretary from 10 people (order matters).\n",
    "    * **Combination:** Choosing a committee of 3 people from 10 people (order doesn't matter).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc48e21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
